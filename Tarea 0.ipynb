{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n",
    "\n",
    "Para la realización de la primera parte de la tarea, se implementó una clase red neuronal basada en SGD con activación de la última capa con softmax, la cual permite mayor modularización su estructura, tanto en tamaño y cantidad de capas, como en la función de activación y costo de ésta.\n",
    "\n",
    "Así se crearon las clases: RedNeuronal, CapaNeuronal y Neurona.\n",
    "\n",
    "Para instanciar una red neuronal se realiza de la siguiente forma:\n",
    "```python\n",
    "nn = RedNeuronal(num_entrada, tam_capas, num_salida, act_inter, cost_func)\n",
    "```\n",
    "Donde los parámetros son:\n",
    ">* **num_entrada**: dimension de los vectores de entrada.\n",
    ">* **tam_capas**: lista de cantidad de neuronas por cada capa interna, [32, 16], también indica la cantidad de capas internas.\n",
    ">* **num_salida**: dimension de los vectores de salida.\n",
    ">* **act_inter**: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\".\n",
    ">* **cost_func**: función de costo: \"crossentropy\" / \"mse\".\n",
    "\n",
    "La implementación que incluye *Forward pass*, *Backward pass* y rutina de entretamiento se encuentra a continuación.\n",
    "\n",
    "Tanto *Forward pass* y *Backward pass* están implementados dentro del método correspondiente a la rutina de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    \n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    # act_inter: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\"\n",
    "    # cost_func: función de costo: \"crossentropy\" / \"mse\" \n",
    "    def __init__(self, num_entrada, tam_capas, num_salida, act_inter, cost_func):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        self.log = dict()\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, act_inter))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], act_inter, self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        if cost_func == \"crossentropy\":\n",
    "            self.cost = self.crossentropy\n",
    "            self.cost_derivative = self.crossentropy_derivative\n",
    "        elif cost_func == \"mse\":\n",
    "            self.cost = self.mse\n",
    "            self.cost_derivative = self.mse_derivative\n",
    "        print(\"Red Neuronal creada\")\n",
    "        print(\"\\tCapas:\", num_entrada, tam_capas, num_salida)\n",
    "        print(\"\\tActivación interna:\", act_inter)\n",
    "        print(\"\\tFunción de costo:\", cost_func)\n",
    "\n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+epsilon)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado, squash_prime, vector_z):\n",
    "        return prediccion-esperado\n",
    "\n",
    "    def mse(self, prediccion, esperado):\n",
    "        return 0.5*np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def mse_derivative(self, prediccion, esperado, squash_prime, vector_z):\n",
    "        return (prediccion-esperado) * squash_prime(vector_z)\n",
    "    \n",
    "    # train(entrada, salida, ciclos, tasa):\n",
    "    # entrada: lista de datos de entrada.\n",
    "    # salida: lista de vectores de salida tipo onehot.\n",
    "    # ciclos: cantidad de epochs.\n",
    "    # tasa: tasa de aprendizaje .\n",
    "    # verbose: Booleano que indica progreso por epoch o solo el final.\n",
    "    # momentum: agrega la posibilidad de entrenar con momentum, recibe el valor de éste.\n",
    "    def train(self, entrada, salida, ciclos, tasa, verbose = False, momentum = None):\n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        tiempo = []\n",
    "        print(\"Comienza entrenamiento de red\", end=\" \")\n",
    "        if momentum:\n",
    "            print(\"con momentum\", momentum)\n",
    "        else:\n",
    "            print()\n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            acertados = 0\n",
    "            primera_iter = True\n",
    "            lista_delta_pesos = list(np.zeros(self.cant_capas))\n",
    "            start = time.time()\n",
    "            for input_v, output in resultList:\n",
    "                #Poblar capa inicial\n",
    "                self.capas[0].set_capa(input_v, input_v) \n",
    "                \n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                prediccion = self.capas[last].get_activaciones()\n",
    "                error_epoch = self.cost(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                \n",
    "                if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                delta = self.cost_derivative(prediccion, np.transpose(output), self.capas[last].squash_derivative, self.capas[last].get_vector_z())\n",
    "                \n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                \n",
    "                delta_peso = np.outer(self.capas[last-1].get_activaciones(), delta)\n",
    "                \n",
    "                if momentum:\n",
    "                    if primera_iter:\n",
    "                        self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                    else:\n",
    "                        self.capas[last].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[last])\n",
    "                    lista_delta_pesos[last] = tasa*delta_peso\n",
    "                else:\n",
    "                    self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    delta_peso = np.outer(self.capas[-l-1].get_activaciones(),delta)\n",
    "                    \n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    if momentum:\n",
    "                        if primera_iter:\n",
    "                            self.capas[-l].actualizar_pesos(delta_peso, tasa)\n",
    "                        else:\n",
    "                            self.capas[-l].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[-l])\n",
    "                        #Actualizacion de lista de pesos\n",
    "                        lista_delta_pesos[-l] = np.outer(self.capas[-l-1].get_activaciones(), delta)\n",
    "                    else:\n",
    "                        self.capas[-l].actualizar_pesos(delta_peso, tasa)\n",
    "                        \n",
    "                primera_iter = True\n",
    "            loss.append(np.mean(errores))\n",
    "            accuracy.append(acertados/len(resultList))\n",
    "            tiempo.append(time.time()-start)\n",
    "            primera_ter = False\n",
    "            if verbose:\n",
    "                print(\"\\tError del epoch \"+str(ciclo)+\": loss\", str(np.round(np.mean(errores),5)),\"- acc\", acertados/len(resultList))\n",
    "        print(\"Resultado de training set de\", ciclos,\"epochs: loss\", np.mean(errores),\"- acc:\", acertados/len(resultList))\n",
    "        self.log['loss'] = loss\n",
    "        self.log['accuracy'] = accuracy\n",
    "        self.log['tiempo'] = tiempo\n",
    "\n",
    "    # test(entrada, salida):\n",
    "    # entrada: lista de datos de entrada de test.\n",
    "    # salida: Lista de datos esperados de test, en formato onehot vector.\n",
    "    # retorna listas de predicciones y valores esperados.\n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        acertados = 0\n",
    "        lista_pred = []\n",
    "        lista_res = []\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            \n",
    "            prediccion = self.capas[last].get_activaciones()\n",
    "            \n",
    "            if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "            lista_pred.append(np.argmax(prediccion))\n",
    "            lista_res.append(np.argmax(output))\n",
    "            # evaluar error final (delta función costo)\n",
    "            error_epoch = self.cost(prediccion, output)\n",
    "            errores.append(error_epoch)\n",
    "        \n",
    "        print(\"Error del test: loss\", str(np.round(np.mean(errores),5)),\"- acc\", acertados/len(resultList))\n",
    "        return lista_pred, lista_res\n",
    "    \n",
    "    # predict(entrada, categorias):\n",
    "    # entrada: Vector de entrada para predecir\n",
    "    # categorias: Etiquetas correspondientes a las categorias\n",
    "    # retorna la etiqueta correspondiente a la predicción de la red neuronal \n",
    "    def predict(self, entrada, categorias):\n",
    "        last = self.cant_capas-1\n",
    "        self.capas[0].set_capa(entrada, entrada)\n",
    "        \n",
    "        for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "        \n",
    "        prediccion = self.capas[last].get_activaciones()\n",
    "        \n",
    "        return categorias[np.argmax(prediccion)]\n",
    "    \n",
    "    # predict(entrada, categorias. epsilon = 0):\n",
    "    # entrada: Vector de entrada para predecir\n",
    "    # categorias: Etiquetas correspondientes a las categorias\n",
    "    # retorna la etiqueta correspondiente a la predicción de la red neuronal \n",
    "    def derivative_test(self, entrada, salida, epsilon = 0):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        pesos_neurona = self.capas[1].neuronas[0].pesos\n",
    "        pesos_neurona += epsilon\n",
    "        self.capas[1].neuronas[0].pesos = pesos_neurona\n",
    "\n",
    "        #Poblar capa inicial\n",
    "        self.capas[0].set_capa(entrada, entrada) \n",
    "        # Feed forward \n",
    "        for j in range(1, len(self.capas)):\n",
    "            self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "\n",
    "        prediccion = self.capas[last].get_activaciones()\n",
    "        \n",
    "        delta = self.cost_derivative(prediccion, np.transpose(salida), self.capas[last].squash_derivative, self.capas[last].get_vector_z())\n",
    "        \n",
    "        for l in range(2, self.cant_capas):\n",
    "            z = self.capas[-l].get_vector_z()\n",
    "            act_prime = self.capas[-l].squash_derivative(z)\n",
    "            delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "            delta_peso = np.outer(self.capas[-l-1].get_activaciones(),delta)\n",
    "        \n",
    "        return self.cost(prediccion, salida), delta_peso[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"arctan\":\n",
    "            self.squash = self.arctan\n",
    "            self.squash_derivative = self.arctan_derivative\n",
    "        elif tipo == \"relu\":\n",
    "            self.squash = self.relu\n",
    "            self.squash_derivative = self.relu_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def arctan(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def arctan_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        x= np.array(x)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        x = np.array(x)\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        X = np.clip(x,-350,350)\n",
    "        x = np.exp(x-np.max(x))\n",
    "        return x / np.sum(x, axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate, momentum = 0, prev_delta_pesos = 0):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz + momentum*prev_delta_pesos\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "        \n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder probar el funcionamiento de la red neuronal se debe instanciar y llamar al método en entrenamiento de la siguiente forma:\n",
    "\n",
    "```python\n",
    "nn.train(entrada, salida, ciclos, tasa)\n",
    "```\n",
    "\n",
    ">* **entrada**: lista de datos de entrada.\n",
    ">* **salida**: lista de vectores de salida tipo onehot.\n",
    ">* **ciclos**: cantidad de epochs.\n",
    ">* **tasa**: tasa de aprendizaje .\n",
    ">* **verbose**: Booleano que indica progreso por epoch o solo el final. (keyword argument, Default: False)\n",
    "\n",
    "Esta función tomará el dataset entregado y ejecutará una rutina de Feed Forward -> Cálculo de error -> Backpropagation por la cantidad de ciclos necesaria, mezclando en cada ciclo los datos de forma aleatoria para minimizar sesgo.\n",
    "\n",
    "### 1.b) Predicciones utilizando feed forward\n",
    "\n",
    "Una vez entrenada la red, se puede utilizar para realizar predicciones de la siguiente forma:\n",
    "```python\n",
    "categorias = ['setosa', 'versicolor', 'verginica']\n",
    "for entrada in x_test:\n",
    "    prediccion = nn.predict(entrada, categorias)\n",
    "```\n",
    "Esta función tomará una entrada e intentará realizar una predicción mediante *Feed Forward*, retornando la etiqueta correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c) Probar la implementación en un problema de clasificación.\n",
    "\n",
    "Para probar nuestra implementación de red neuronal se utliza el dataset iris el cual se normaliza y se separa en un *training set* y un *testing set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga y estructuración de datos\n",
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=50)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "tags = ['setosa', 'versicolor', 'verginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos cargados se procede a instanciar las 4 redes neuronales a evaluar, utilizando *sigmoid* o ReLU como función de activación y usando MSE o Cross Entropy como función de costo:\n",
    "\n",
    "Las funciones de activación están implementadas dentro de la clase CapaNeuronal junto con sus derivadas.\n",
    "\n",
    "Las funciones de costo están implementadas dentro de la clase RedNeuronal junto con sus derivadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar redes neuronales\n",
    "nn_sm = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "nn_sc = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"crossentropy\")\n",
    "nn_rm = RedNeuronal(4, [32, 16], 3, \"relu\", \"mse\")\n",
    "nn_rc = RedNeuronal(4, [32, 16], 3, \"relu\", \"crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se procede a entrenar cada una de las redes con 100 *epochs* y un *learning rate* $\\eta = 0.1$\n",
    "\n",
    "Tras esto se extraen los datos del registro de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Entrenar la red \n",
    "#nn_sm.train(x_train, y_onehot_train, 100, 0.1)\n",
    "#nn_sc.train(x_train, y_onehot_train, 100, 0.1)\n",
    "nn_rm.train(x_train, y_onehot_train, 100, 0.1)\n",
    "nn_rc.train(x_train, y_onehot_train, 100, 0.1)\n",
    "\n",
    "loss_sm = nn_sm.log['loss']\n",
    "accuracy_sm = nn_sm.log['accuracy']\n",
    "tiempo_sm = nn_sm.log['tiempo']\n",
    "\n",
    "loss_sc = nn_sc.log['loss']\n",
    "accuracy_sc = nn_sc.log['accuracy']\n",
    "tiempo_sc = nn_sc.log['tiempo']\n",
    "\n",
    "loss_rm = nn_rm.log['loss']\n",
    "accuracy_rm = nn_rm.log['accuracy']\n",
    "tiempo_rm = nn_rm.log['tiempo']\n",
    "\n",
    "loss_rc = nn_rc.log['loss']\n",
    "accuracy_rc = nn_rc.log['accuracy']\n",
    "tiempo_rc = nn_rc.log['tiempo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras entrenar las redes, se procede a graficar la función objetivo \"Loss\" y la precisión de clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(15,8))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(len(accuracy_sm)), accuracy_sm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_sm)), loss_sm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(222)\n",
    "plt.plot(range(len(accuracy_sc)), accuracy_sc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_sc)), loss_sc, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(223)\n",
    "plt.plot(range(len(accuracy_rm)), accuracy_rm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_rm)), loss_rm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(224)\n",
    "plt.plot(range(len(accuracy_rc)), accuracy_rc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_rc)), loss_rc, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tiempo total para activación sigmoid y costo MSE\", round(np.sum(tiempo_sm),3))\n",
    "print(\"Tiempo total para activación sigmoid y costo Cross Entropy\", round(np.sum(tiempo_sc),3))\n",
    "print(\"Tiempo total para activación ReLU y costo MSE\", round(np.sum(tiempo_rm),3))\n",
    "print(\"Tiempo total para activación ReLU y costo Cross Entropy\", round(np.sum(tiempo_rc),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver como la configuración Sigmoid y Cross-Entropy entrega resultados notoriamente mejores tanto para precisión de clasificación y minización de la función de pérdida, obteniéndose en algunos casos una accuracy cercana al 99%.\n",
    "\n",
    "Además se desprende que la combinación ReLU y Cross Entropy es la más rápida, siendo cada una de estas la más rápida de su categoría (activación y costo) con un aumento de más de 4 veces la velocidad repecto a la configuración más lenta.\n",
    "\n",
    "### Matriz de confusión para test con red de activación sigmoide y costo Cross-Entropy\n",
    "\n",
    "Además de las funciones mencionadas previamente, se creó un método para hacer una rutina de testing y evaluar loss y accuracy con un dataset desconocido para la red neuronal:\n",
    "\n",
    "```python\n",
    "nn.test(entrada, salida)\n",
    "```\n",
    "Los parámetros de la función son\n",
    ">* entrada: lista de datos de entrada de test\n",
    ">* salida: Lista de datos esperados de test, en formato onehot vector\n",
    "\n",
    "retorna listas de resultados predichos y esperados para poder realizar un análisis de confusión.\n",
    "\n",
    "al probar esto se tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Resultado real')\n",
    "    plt.xlabel('Resultado predicho')\n",
    "    \n",
    "pred, real = nn_sc.test(x_test, y_onehot_test)\n",
    "cnf_matrix = confusion_matrix(real, pred)\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cnf_matrix, classes=tags, normalize=True,\n",
    "                      title='Matriz de confusión para red con Sigmoid y Cross-Entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la matriz se desprende que solo un pequeño porcentaje del test falló, ya que la red predijo que era una planta del tipo \"Verginica\" cuadno en realidad era de tipo \"Versicolor\".\n",
    "\n",
    "Fuera de esto, el resto de las predicciones no fallaron obteniéndose un 100% de precisión en algunos casos, como se ve en la diagonal de la matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREGUNTA 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Neuronal creada\n",
      "\tCapas: 4 [32, 16] 3\n",
      "\tActivación interna: sigmoid\n",
      "\tFunción de costo: mse\n",
      "Verificación de derivadas para MSE\n",
      "Por definición: 1.51605534793e-08 | Derivada normal: 1.85164875396e-07\n",
      "\n",
      "Red Neuronal creada\n",
      "\tCapas: 4 [32, 16] 3\n",
      "\tActivación interna: sigmoid\n",
      "\tFunción de costo: crossentropy\n",
      "Verificación de derivadas para Cross-Entropy\n",
      "Por definición: 9.24933787338e-08 | Derivada normal: 8.80566589154e-07\n"
     ]
    }
   ],
   "source": [
    "nn_prime = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "epsilon = .5\n",
    "diff, deltapeso = nn_prime.derivative_test(x_test[0], y_onehot_test[0])\n",
    "diff_epsilon, _ = nn_prime.derivative_test(x_test[0], y_onehot_test[0], epsilon)\n",
    "\n",
    "derivative = (diff_epsilon-diff)/epsilon\n",
    "\n",
    "print(\"Verificación de derivadas para MSE\")\n",
    "print(\"Por definición:\", derivative, \"| Derivada normal:\", deltapeso[0])\n",
    "print()\n",
    "nn_prime = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"crossentropy\")\n",
    "epsilon = .5\n",
    "diff, deltapeso = nn_prime.derivative_test(x_test[0], y_onehot_test[0])\n",
    "diff_epsilon, _ = nn_prime.derivative_test(x_test[0], y_onehot_test[0], epsilon)\n",
    "\n",
    "derivative = (diff_epsilon-diff)/epsilon\n",
    "print(\"Verificación de derivadas para Cross-Entropy\")\n",
    "print(\"Por definición:\", derivative, \"| Derivada normal:\", deltapeso[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
