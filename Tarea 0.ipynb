{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #se importa numpy como np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    def __init__(self, num_entrada, tam_capas, num_salida):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        \n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+1e-9)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado):\n",
    "        return -(esperado-1/prediccion+(1+esperado)*(1/(1-prediccion)))    \n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        return np.array(res)\n",
    "    \n",
    "    def train(self, entrada, output, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        for ciclo in range(ciclos):\n",
    "            iteracion = 0\n",
    "            errores_epoch = 0 # Error general del ciclo. Almacena un escalar por cada entrada\n",
    "            errores_derivadas = np.zeros(self.num_salida) # Error de crossentropy()' almanacena por neurona final el error\n",
    "            error_softmax = np.zeros(16) # Error entre capa de salida y ultima capa oculta\n",
    "            error_capa2 = np.zeros(16) # ALmacena activacion de cada neurona de la ultima capa oculta\n",
    "            # Comienza 1 epoch\n",
    "            for i in range(len(entrada)):\n",
    "                i+=1\n",
    "                self.capas[0].set_capa(entrada[i]) #Poblar capa inicial\n",
    "                \n",
    "                # Feed forward\n",
    "                for j in range(1, len(self.capas)-1):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                self.capas[last].softmax_forward(self.capas[last-1])\n",
    "                \n",
    "                # evaluar error\n",
    "                errores_epoch += self.crossentropy(self.capas[last].get_activaciones(), output[i])                    \n",
    "                errores_derivadas += self.crossentropy_derivative(self.capas[last].get_activaciones(), output[i])\n",
    "                error_softmax += self.softmax_derivative(self.capas[last-1].get_activaciones())\n",
    "                error_capa2 += self.capas[last-1].get_activaciones()\n",
    "                #self.capas[last].mostrar_capa()\n",
    "            #Error final de cada epoch    \n",
    "            error_final = errores_epoch/iteracion\n",
    "            ce_derivative = errores_derivadas/iteracion #promedio de errores por neurona de salida\n",
    "            softmax_derivative = error_softmax/iteracion #promedio errores por pesos de capa de salida segun capa anterior\n",
    "            error_capa2 = error_capa2/iteracion #promedio de activaciones de la capa 2\n",
    "            error_capa2 = np.transpose(np.tile(error_capa2, (3,1)))\n",
    "            print(\"Error del epoch: \"+str(error_final))\n",
    "            error = np.multiply(np.outer(softmax_derivative,ce_derivative),error_capa2)\n",
    "            #pesos actualizados de la ULTIMA CAPA (SALIDA)\n",
    "            self.capas[last].actualizar_pesos(error, tasa)\n",
    "            # actualiacion de pesos capas ocultas\n",
    "            \n",
    "                # Back propagation (actualizar pesos de las neuronas)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Felipe Code\n",
    "\n",
    "def set_peso(self,tasa,derivada): #neurona.pesos\n",
    "    return neurona.pesos - tasa*derivada\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        for neurona in self.neuronas:\n",
    "            activaciones = capa_anterior.get_activaciones()\n",
    "            pesos = neurona.pesos\n",
    "            result = self.sigmoid(np.dot(pesos, activaciones) + self.bias)          \n",
    "            neurona.set_activacion(result)\n",
    "    \n",
    "    def softmax_forward(self, capa_anterior):\n",
    "        pre_softmax = []\n",
    "        for neurona in self.neuronas:\n",
    "            activaciones = capa_anterior.get_activaciones()\n",
    "            pesos = neurona.pesos\n",
    "            pre_softmax.append(np.dot(pesos, activaciones) + self.bias)\n",
    "        self.set_capa(self.softmax(pre_softmax))\n",
    "    \n",
    "    def actualizar_pesos(self, matriz, rate):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos += rate*matriz #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, valores):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(valores[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "\n",
    "    def set_activacion(self, valor):\n",
    "        self.activacion = valor\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import keras\n",
    "x_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "y_onehot = keras.utils.to_categorical(y_train)\n",
    "#transform target to one hot vector\n",
    "nn = RedNeuronal(4, [32, 16], 3)\n",
    "nn.train(x_train, y_onehot, 1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_onehot, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
