{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n",
    "\n",
    "Para la realización de la primera parte de la tarea, se implementó una clase red neuronal basada en SGD con activación de la última capa con softmax, la cual permite mayor modularización su estructura, tanto en tamaño y cantidad de capas, como en la función de activación y costo de ésta.\n",
    "\n",
    "Así se crearon las clases: RedNeuronal, CapaNeuronal y Neurona.\n",
    "\n",
    "Para instanciar una red neuronal se realiza de la siguiente forma:\n",
    "```python\n",
    "nn = RedNeuronal(num_entrada, tam_capas, num_salida, act_inter, cost_func)\n",
    "```\n",
    "Donde los parámetros son:\n",
    ">* **num_entrada**: dimension de los vectores de entrada.\n",
    ">* **tam_capas**: lista de cantidad de neuronas por cada capa interna, [32, 16].\n",
    ">* **num_salida**: dimension de los vectores de salida.\n",
    ">* **act_inter**: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\".\n",
    ">* **cost_func**: función de costo: \"crossentropy\" / \"mse\".\n",
    "\n",
    "La implementación que incluye *Forward pass*, *Backward pass* y rutina de entretamiento se encuentra a continuación.\n",
    "\n",
    "Tanto *Forward pass* y *Backward pass* están implementados dentro del método correspondiente a la rutina de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    \n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    # act_inter: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\"\n",
    "    # cost_func: función de costo: \"crossentropy\" / \"mse\" \n",
    "    def __init__(self, num_entrada, tam_capas, num_salida, act_inter, cost_func):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        self.log = dict()\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, act_inter))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], act_inter, self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        if cost_func == \"crossentropy\":\n",
    "            self.cost = self.crossentropy\n",
    "            self.cost_derivative = self.crossentropy_derivative\n",
    "        elif cost_func == \"mse\":\n",
    "            self.cost = self.mse\n",
    "            self.cost_derivative = self.mse_derivative\n",
    "        print(\"Red Neuronal creada\")\n",
    "        print(\"\\tCapas:\", num_entrada, tam_capas, num_salida)\n",
    "        print(\"\\tActivación interna:\", act_inter)\n",
    "        print(\"\\tFunción de costo:\", cost_func)\n",
    "\n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+epsilon)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado, squash_prime, vector_z):\n",
    "        return prediccion-esperado\n",
    "\n",
    "    def mse(self, prediccion, esperado):\n",
    "        return 0.5*np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def mse_derivative(self, prediccion, esperado, squash_prime, vector_z):\n",
    "        return (prediccion-esperado) * squash_prime(vector_z)\n",
    "    \n",
    "    # train(entrada, salida, ciclos, tasa):\n",
    "    # entrada: lista de datos de entrada.\n",
    "    # salida: lista de vectores de salida tipo onehot.\n",
    "    # ciclos: cantidad de epochs.\n",
    "    # tasa: tasa de aprendizaje .\n",
    "    # verbose: Booleano que indica progreso por epoch o solo el final.\n",
    "    # momentum: agrega la posibilidad de entrenar con momentum, recibe el valor de éste.\n",
    "    def train(self, entrada, salida, ciclos, tasa, verbose = False, momentum = None):\n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        tiempo = []\n",
    "        print(\"Comienza entrenamiento de red\", end=\" \")\n",
    "        if momentum:\n",
    "            print(\"con momentum\", momentum)\n",
    "        else:\n",
    "            print()\n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            acertados = 0\n",
    "            primera_iter = True\n",
    "            lista_delta_pesos = list(np.zeros(self.cant_capas))\n",
    "            start = time.time()\n",
    "            for input_v, output in resultList:\n",
    "                #Poblar capa inicial\n",
    "                self.capas[0].set_capa(input_v, input_v) \n",
    "                \n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                prediccion = self.capas[last].get_activaciones()\n",
    "                error_epoch = self.cost(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                \n",
    "                if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                delta = self.cost_derivative(prediccion, np.transpose(output), self.capas[last].squash_derivative, self.capas[last].get_vector_z())\n",
    "                \n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                \n",
    "                delta_peso = np.outer(self.capas[last-1].get_activaciones(), delta)\n",
    "                \n",
    "                if momentum:\n",
    "                    if primera_iter:\n",
    "                        self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                    else:\n",
    "                        self.capas[last].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[last])\n",
    "                    lista_delta_pesos[last] = tasa*delta_peso\n",
    "                else:\n",
    "                    self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    delta_peso = np.outer(self.capas[-l-1].get_activaciones(),delta)\n",
    "                    \n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    if momentum:\n",
    "                        if primera_iter:\n",
    "                            self.capas[-l].actualizar_pesos(delta_peso, tasa)\n",
    "                        else:\n",
    "                            self.capas[-l].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[-l])\n",
    "                        #Actualizacion de lista de pesos\n",
    "                        lista_delta_pesos[-l] = np.outer(self.capas[-l-1].get_activaciones(), delta)\n",
    "                    else:\n",
    "                        self.capas[-l].actualizar_pesos(delta_peso, tasa)\n",
    "                        \n",
    "                primera_iter = True\n",
    "            loss.append(np.mean(errores))\n",
    "            accuracy.append(acertados/len(resultList))\n",
    "            tiempo.append(time.time()-start)\n",
    "            primera_ter = False\n",
    "            if verbose:\n",
    "                print(\"\\tError del epoch \"+str(ciclo)+\": loss\", str(np.round(np.mean(errores),5)),\"- acc\", acertados/len(resultList))\n",
    "        print(\"Resultado de training set de\", ciclos,\"epochs: loss\", np.mean(errores),\"- acc:\", acertados/len(resultList))\n",
    "        self.log['loss'] = loss\n",
    "        self.log['accuracy'] = accuracy\n",
    "        self.log['tiempo'] = tiempo\n",
    "\n",
    "    # test(entrada, salida):\n",
    "    # entrada: lista de datos de entrada de test\n",
    "    # salida: Lista de datos esperados de test, en formato onehot vector\n",
    "    # retorna loss y accuracy\n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        acertados = 0\n",
    "        lista_pred = []\n",
    "        lista_res = []\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            \n",
    "            prediccion = self.capas[last].get_activaciones()\n",
    "            \n",
    "            if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "            lista_pred.append(np.argmax(prediccion))\n",
    "            lista_res.append(np.argmax(output))\n",
    "            # evaluar error final (delta función costo)\n",
    "            error_epoch = self.crossentropy(prediccion, output)\n",
    "            errores.append(error_epoch)\n",
    "        \n",
    "        print(\"Error del test: loss\", str(np.round(np.mean(errores),5)),\"- acc\", acertados/len(resultList))\n",
    "        return lista_pred, lista_res\n",
    "    \n",
    "    # predict(entrada, categorias):\n",
    "    # entrada: Vector de entrada para predecir\n",
    "    # categorias: Etiquetas correspondientes a las categorias\n",
    "    # retorna la etiqueta correspondiente a la predicción de la red neuronal \n",
    "    def predict(self, entrada, categorias):\n",
    "        last = self.cant_capas-1\n",
    "        self.capas[0].set_capa(entrada, entrada)\n",
    "        \n",
    "        for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "        \n",
    "        prediccion = self.capas[last].get_activaciones()\n",
    "        \n",
    "        return categorias[np.argmax(prediccion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"arctan\":\n",
    "            self.squash = self.arctan\n",
    "            self.squash_derivative = self.arctan_derivative\n",
    "        elif tipo == \"relu\":\n",
    "            self.squash = self.relu\n",
    "            self.squash_derivative = self.relu_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def arctan(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def arctan_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        x= np.array(x)\n",
    "        return np.maximum(x, 0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        x = np.array(x)\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.exp(x-np.max(x))\n",
    "        return x / np.sum(x, axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate, momentum = 0, prev_delta_pesos = 0):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz + momentum*prev_delta_pesos\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "        \n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder probar el funcionamiento de la red neuronal se debe instanciar y llamar al método en entrenamiento de la siguiente forma:\n",
    "\n",
    "```python\n",
    "nn.train(entrada, salida, ciclos, tasa)\n",
    "```\n",
    "\n",
    ">* **entrada**: lista de datos de entrada.\n",
    ">* **salida**: lista de vectores de salida tipo onehot.\n",
    ">* **ciclos**: cantidad de epochs.\n",
    ">* **tasa**: tasa de aprendizaje .\n",
    ">* **verbose**: Booleano que indica progreso por epoch o solo el final. (keyword argument)\n",
    "\n",
    "Esta función tomará el dataset entregado y ejecutará una rutina de Feed Forward -> Cálculo de error -> Backpropagation por la cantidad de ciclos necesaria, mezclando en cada ciclo los datos de forma aleatoria para minimizar sesgo.\n",
    "\n",
    "1.b)\n",
    "\n",
    "Una vez entrenada la red, se puede utilizar para realizar predicciones de la siguiente forma:\n",
    "```python\n",
    "categorias = ['setosa', 'versicolor', 'verginica']\n",
    "for entrada in x_test:\n",
    "    prediccion = nn.predict(entrada, categorias)\n",
    "```\n",
    "Esta función tomará una entrada e intentará realizar una predicción mediante *Feed Forward*, retornándola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c) Probar la implementación en un problema de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga y estructuración de datos\n",
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=50)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "tags = ['setosa', 'versicolor', 'verginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos cargados se procede a instanciar las 4 redes neuronales a evaluar, utilizando *sigmoid* o ReLU como función de activación y usando MSE o Cross Entropy como función costo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar red neuronal\n",
    "nn_sm = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "nn_sc = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"crossentropy\")\n",
    "nn_rm = RedNeuronal(4, [32, 16], 3, \"relu\", \"mse\")\n",
    "nn_rc = RedNeuronal(4, [32, 16], 3, \"relu\", \"crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Entrenar la red \n",
    "nn_sm.train(x_train, y_onehot_train, 100, 0.1)\n",
    "nn_sc.train(x_train, y_onehot_train, 100, 0.1)\n",
    "nn_rm.train(x_train, y_onehot_train, 100, 0.1)\n",
    "nn_rc.train(x_train, y_onehot_train, 100, 0.1)\n",
    "loss_sm = nn_sm.log['loss']\n",
    "accuracy_sm = nn_sm.log['accuracy']\n",
    "tiempo_sm = nn_sm.log['tiempo']\n",
    "\n",
    "#nn_sc.train(x_train, y_onehot_train, 100, 0.1, verbose = False)\n",
    "#Testear\n",
    "print(\"\\nTESTING\")\n",
    "nn_sm.test(x_test, y_onehot_test)\n",
    "#nn_sc.test(x_test, y_onehot_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = x_test[5]\n",
    "\n",
    "print(\"Probando para\", tags[np.argmax(y_onehot_test[5])])\n",
    "print(\"Predicción de NN:\",nn_sm.predict(x_predict, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Resultado real')\n",
    "    plt.xlabel('Resultado predicho')\n",
    "    \n",
    "pred, real = nn_sm.test(x_test, y_onehot_test)\n",
    "cnf_matrix = confusion_matrix(real, pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=tags, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(15,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(accuracy_sm)), accuracy_sm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_sm)), loss_sm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(accuracy_sc)), accuracy_sc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_sc)), loss_sc, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_onehot_train, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
