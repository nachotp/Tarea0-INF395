{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #se importa numpy como np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    def __init__(self, num_entrada, tam_capas, num_salida):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, \"sigmoid\"))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], \"sigmoid\", self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        \n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+1e-9)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado):\n",
    "        return -(esperado-1/prediccion+(1+esperado)*(1/(1-prediccion)))    \n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        return np.array(res)\n",
    "    \n",
    "    def train(self, entrada, output, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        for ciclo in range(ciclos):\n",
    "            print(\"Epoch: \", ciclo)\n",
    "            iteracion = 0\n",
    "            errores_epoch = 0 # Error general del ciclo. Almacena un escalar por cada entrada\n",
    "            errores_derivadas = np.zeros(self.num_salida) # Error de crossentropy()' almanacena por neurona final el error\n",
    "            error_softmax = np.zeros(16) # Error entre capa de salida y ultima capa oculta\n",
    "            error_capa2 = np.zeros(16) # ALmacena activacion de cada neurona de la ultima capa oculta\n",
    "            # Comienza 1 epoch\n",
    "            for i in range(len(entrada)):\n",
    "                iteracion+=1\n",
    "                self.capas[0].set_capa(entrada[i]) #Poblar capa inicial\n",
    "                \n",
    "                # Feed forward\n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error\n",
    "                errores_epoch += self.crossentropy(self.capas[last].get_activaciones(), output[i])                    \n",
    "                errores_derivadas += self.crossentropy_derivative(self.capas[last].get_activaciones(), output[i])\n",
    "                \n",
    "                error_softmax += self.softmax_derivative(self.capas[last-1].get_activaciones()) # CHECK\n",
    "                error_capa2 += self.capas[last-1].get_activaciones()\n",
    "                #self.capas[last].mostrar_capa()\n",
    "            #Error final de cada epoch\n",
    "            for c in self.capas:\n",
    "                c.epoch_ponderar(iteracion)\n",
    "            error_final = errores_epoch/iteracion\n",
    "            ce_derivative = errores_derivadas/iteracion #promedio de errores por neurona de salida\n",
    "            softmax_derivative = error_softmax/iteracion #promedio errores por pesos de capa de salida segun capa anterior\n",
    "            error_capa2 = error_capa2/iteracion #promedio de activaciones de la capa 2\n",
    "            error_capa2 = np.transpose(np.tile(error_capa2, (3,1)))\n",
    "            print(\"Error del epoch: \"+str(error_final))\n",
    "            error = np.multiply(np.outer(softmax_derivative,ce_derivative),error_capa2)\n",
    "            #pesos actualizados de la ULTIMA CAPA (SALIDA)\n",
    "            self.capas[last].actualizar_pesos(error, tasa)\n",
    "            # actualiacion de pesos capas ocultas\n",
    "            \n",
    "            capa1 = self.capas[1].acum_act\n",
    "            capa1_sigmoid = self.capas[2].error_act\n",
    "            print(capa1)\n",
    "            print(capa1_sigmoid)\n",
    "            capa0 = self.capas[0].acum_act\n",
    "            capa0_sigmoid = self.capas[1].error_act\n",
    "            print(capa0)\n",
    "            print(capa0_sigmoid)\n",
    "            for c in self.capas:\n",
    "                c.epoch_clear()\n",
    "                # Back propagation (actualizar pesos de las neuronas)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        self.acum_act = np.zeros(tam_capa) #acumular activaciones de la capa por epoch\n",
    "        self.error_act = np.zeros(tam_anterior) #PENDING\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "    \n",
    "    def epoch_clear(self):\n",
    "        self.acum_act = np.zeros(self.tam_capa) #acumular activaciones de la capa por epoch\n",
    "        self.error_act = np.zeros(self.tam_anterior) #PENDING\n",
    "        \n",
    "    def epoch_ponderar(self, n):\n",
    "        self.acum_act /= n\n",
    "        self.error_act /= n\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([1 / (1 + np.exp(-i)) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        for neurona in self.neuronas:\n",
    "            activaciones = capa_anterior.get_activaciones()\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + self.bias)\n",
    "        self.error_act += self.squash_derivative(activaciones) # Errores de activacion capa anterior\n",
    "        self.acum_act += self.squash(pre_squash) # acumulacion de activaciones de la capa\n",
    "        self.set_capa(self.squash(pre_squash))\n",
    "    \n",
    "    def actualizar_pesos(self, matriz, rate):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos += rate*matriz #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, valores):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(valores[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "\n",
    "    def set_activacion(self, valor):\n",
    "        self.activacion = valor\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Error del epoch: 0.481357699142\n",
      "[ 0.6394412   0.67602149  0.65494037  0.62044272  0.66669204  0.70314203\n",
      "  0.6869769   0.6619335   0.69915845  0.69834379  0.69708367  0.68432224\n",
      "  0.65510742  0.6753968   0.62865233  0.69676341  0.65327128  0.66161257\n",
      "  0.65872751  0.65048266  0.66595859  0.66963962  0.64765544  0.62534032\n",
      "  0.68334158  0.63886304  0.64179547  0.636999    0.69516042  0.6472425\n",
      "  0.65383435  0.65836478]\n",
      "[ 0.22168436  0.22150771  0.22207087  0.22169411  0.22174086  0.22056175\n",
      "  0.22089611  0.22134301  0.22070447  0.22065202  0.22081229  0.22114138\n",
      "  0.22192359  0.22154077  0.22180399  0.22080418  0.22194121  0.22189809\n",
      "  0.22151595  0.22221923  0.22181978  0.22167189  0.22223138  0.22153716\n",
      "  0.22127546  0.22242664  0.22238529  0.22161303  0.22086983  0.22224739\n",
      "  0.22210193  0.22189367]\n",
      "[ 0.  0.  0.  0.]\n",
      "[ 0.20453805  0.20779391  0.20075385  0.20118694]\n",
      "Epoch:  1\n",
      "Error del epoch: 0.378177910694\n",
      "[ 0.6394412   0.67602149  0.65494037  0.62044272  0.66669204  0.70314203\n",
      "  0.6869769   0.6619335   0.69915845  0.69834379  0.69708367  0.68432224\n",
      "  0.65510742  0.6753968   0.62865233  0.69676341  0.65327128  0.66161257\n",
      "  0.65872751  0.65048266  0.66595859  0.66963962  0.64765544  0.62534032\n",
      "  0.68334158  0.63886304  0.64179547  0.636999    0.69516042  0.6472425\n",
      "  0.65383435  0.65836478]\n",
      "[ 0.22168436  0.22150771  0.22207087  0.22169411  0.22174086  0.22056175\n",
      "  0.22089611  0.22134301  0.22070447  0.22065202  0.22081229  0.22114138\n",
      "  0.22192359  0.22154077  0.22180399  0.22080418  0.22194121  0.22189809\n",
      "  0.22151595  0.22221923  0.22181978  0.22167189  0.22223138  0.22153716\n",
      "  0.22127546  0.22242664  0.22238529  0.22161303  0.22086983  0.22224739\n",
      "  0.22210193  0.22189367]\n",
      "[ 0.  0.  0.  0.]\n",
      "[ 0.20453805  0.20779391  0.20075385  0.20118694]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import keras\n",
    "x_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "y_onehot = keras.utils.to_categorical(y_train)\n",
    "#transform target to one hot vector\n",
    "nn = RedNeuronal(4, [32, 16], 3)\n",
    "nn.train(x_train, y_onehot, 2, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_onehot, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
