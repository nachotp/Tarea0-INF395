{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #se importa numpy como np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    def __init__(self, num_entrada, tam_capas, num_salida):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada))\n",
    "        for i in range(1, len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i-1], self.capas[i-1].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, self.capas[len(self.capas)-1].tam_capa))\n",
    "        \n",
    "    def train(self, entrada, output, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        for ciclo in range(ciclos):\n",
    "            for vector in entrada:\n",
    "                self.capas[0].set_capa(vector) #Poblar capa inicial\n",
    "                # Feed forward\n",
    "                for i in range(1, len(self.capas)):\n",
    "                    self.capas[i].feed_forward(self.capas[i-1])\n",
    "                \n",
    "                self.capas[len(self.capas)-1].mostrar_capa()\n",
    "                \n",
    "                # evaluar error\n",
    "\n",
    "                # Back propagation (actualizar pesos de las neuronas)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior)] * tam_capa # lista de neuronas de la capa\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def derivate_sigmoid(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return [neurona.activacion for neurona in self.neuronas]\n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        for neurona in self.neuronas:\n",
    "            activaciones = capa_anterior.get_activaciones()\n",
    "            pesos = neurona.pesos\n",
    "            result = self.sigmoid(np.dot(pesos, activaciones) + self.bias)          \n",
    "            neurona.set_activacion(result)\n",
    "            \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, valores):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(valores[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        \n",
    "    def set_activacion(self, valor):\n",
    "        self.activacion = valor\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.997402799338\tActivación: 0.997402799338\tActivación: 0.997402799338\t\n",
      "-----------------------------\n",
      "Activación: 0.994221330411\tActivación: 0.994221330411\tActivación: 0.994221330411\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.978650163767\tActivación: 0.978650163767\tActivación: 0.978650163767\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.978650163767\tActivación: 0.978650163767\tActivación: 0.978650163767\t\n",
      "-----------------------------\n",
      "Activación: 0.978650163767\tActivación: 0.978650163767\tActivación: 0.978650163767\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.997402799338\tActivación: 0.997402799338\tActivación: 0.997402799338\t\n",
      "-----------------------------\n",
      "Activación: 0.997402799338\tActivación: 0.997402799338\tActivación: 0.997402799338\t\n",
      "-----------------------------\n",
      "Activación: 0.994221330411\tActivación: 0.994221330411\tActivación: 0.994221330411\t\n",
      "-----------------------------\n",
      "Activación: 0.994221330411\tActivación: 0.994221330411\tActivación: 0.994221330411\t\n",
      "-----------------------------\n",
      "Activación: 0.994221330411\tActivación: 0.994221330411\tActivación: 0.994221330411\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.997402799338\tActivación: 0.997402799338\tActivación: 0.997402799338\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.998931222498\tActivación: 0.998931222498\tActivación: 0.998931222498\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.997402799338\tActivación: 0.997402799338\tActivación: 0.997402799338\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.997402799338\tActivación: 0.997402799338\tActivación: 0.997402799338\t\n",
      "-----------------------------\n",
      "Activación: 0.978650163767\tActivación: 0.978650163767\tActivación: 0.978650163767\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.978650163767\tActivación: 0.978650163767\tActivación: 0.978650163767\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.978650163767\tActivación: 0.978650163767\tActivación: 0.978650163767\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.994221330411\tActivación: 0.994221330411\tActivación: 0.994221330411\t\n",
      "-----------------------------\n",
      "Activación: 0.994221330411\tActivación: 0.994221330411\tActivación: 0.994221330411\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.999591065413\tActivación: 0.999591065413\tActivación: 0.999591065413\t\n",
      "-----------------------------\n",
      "Activación: 0.997402799338\tActivación: 0.997402799338\tActivación: 0.997402799338\t\n",
      "-----------------------------\n",
      "Activación: 0.994221330411\tActivación: 0.994221330411\tActivación: 0.994221330411\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.988328558682\tActivación: 0.988328558682\tActivación: 0.988328558682\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999999906563\tActivación: 0.999999906563\tActivación: 0.999999906563\t\n",
      "-----------------------------\n",
      "Activación: 0.999993044584\tActivación: 0.999993044584\tActivación: 0.999993044584\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.999993044584\tActivación: 0.999993044584\tActivación: 0.999993044584\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999993044584\tActivación: 0.999993044584\tActivación: 0.999993044584\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999993044584\tActivación: 0.999993044584\tActivación: 0.999993044584\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999997275554\tActivación: 0.999997275554\tActivación: 0.999997275554\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999998846169\tActivación: 0.999998846169\tActivación: 0.999998846169\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.999999937972\tActivación: 0.999999937972\tActivación: 0.999999937972\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999993044584\tActivación: 0.999993044584\tActivación: 0.999993044584\t\n",
      "-----------------------------\n",
      "Activación: 0.999997275554\tActivación: 0.999997275554\tActivación: 0.999997275554\t\n",
      "-----------------------------\n",
      "Activación: 0.999993044584\tActivación: 0.999993044584\tActivación: 0.999993044584\t\n",
      "-----------------------------\n",
      "Activación: 0.999998846169\tActivación: 0.999998846169\tActivación: 0.999998846169\t\n",
      "-----------------------------\n",
      "Activación: 0.999999906563\tActivación: 0.999999906563\tActivación: 0.999999906563\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999999906563\tActivación: 0.999999906563\tActivación: 0.999999906563\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999998846169\tActivación: 0.999998846169\tActivación: 0.999998846169\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.999998846169\tActivación: 0.999998846169\tActivación: 0.999998846169\t\n",
      "-----------------------------\n",
      "Activación: 0.999993044584\tActivación: 0.999993044584\tActivación: 0.999993044584\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999998846169\tActivación: 0.999998846169\tActivación: 0.999998846169\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999997275554\tActivación: 0.999997275554\tActivación: 0.999997275554\t\n",
      "-----------------------------\n",
      "Activación: 0.999999465292\tActivación: 0.999999465292\tActivación: 0.999999465292\t\n",
      "-----------------------------\n",
      "Activación: 0.999999985411\tActivación: 0.999999985411\tActivación: 0.999999985411\t\n",
      "-----------------------------\n",
      "Activación: 0.999999966457\tActivación: 0.999999966457\tActivación: 0.999999966457\t\n",
      "-----------------------------\n",
      "Activación: 0.999999977637\tActivación: 0.999999977637\tActivación: 0.999999977637\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999980659\tActivación: 0.999999980659\tActivación: 0.999999980659\t\n",
      "-----------------------------\n",
      "Activación: 0.999999977637\tActivación: 0.999999977637\tActivación: 0.999999977637\t\n",
      "-----------------------------\n",
      "Activación: 0.999999937972\tActivación: 0.999999937972\tActivación: 0.999999937972\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999985411\tActivación: 0.999999985411\tActivación: 0.999999985411\t\n",
      "-----------------------------\n",
      "Activación: 0.9999999732\tActivación: 0.9999999732\tActivación: 0.9999999732\t\n",
      "-----------------------------\n",
      "Activación: 0.999999966457\tActivación: 0.999999966457\tActivación: 0.999999966457\t\n",
      "-----------------------------\n",
      "Activación: 0.999999977637\tActivación: 0.999999977637\tActivación: 0.999999977637\t\n",
      "-----------------------------\n",
      "Activación: 0.9999999732\tActivación: 0.9999999732\tActivación: 0.9999999732\t\n",
      "-----------------------------\n",
      "Activación: 0.999999984299\tActivación: 0.999999984299\tActivación: 0.999999984299\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999980659\tActivación: 0.999999980659\tActivación: 0.999999980659\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.9999999732\tActivación: 0.9999999732\tActivación: 0.9999999732\t\n",
      "-----------------------------\n",
      "Activación: 0.9999999732\tActivación: 0.9999999732\tActivación: 0.9999999732\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999977637\tActivación: 0.999999977637\tActivación: 0.999999977637\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999977637\tActivación: 0.999999977637\tActivación: 0.999999977637\t\n",
      "-----------------------------\n",
      "Activación: 0.999999906563\tActivación: 0.999999906563\tActivación: 0.999999906563\t\n",
      "-----------------------------\n",
      "Activación: 0.999999966457\tActivación: 0.999999966457\tActivación: 0.999999966457\t\n",
      "-----------------------------\n",
      "Activación: 0.9999999732\tActivación: 0.9999999732\tActivación: 0.9999999732\t\n",
      "-----------------------------\n",
      "Activación: 0.999999980659\tActivación: 0.999999980659\tActivación: 0.999999980659\t\n",
      "-----------------------------\n",
      "Activación: 0.999999847376\tActivación: 0.999999847376\tActivación: 0.999999847376\t\n",
      "-----------------------------\n",
      "Activación: 0.99999972748\tActivación: 0.99999972748\tActivación: 0.99999972748\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.999999984299\tActivación: 0.999999984299\tActivación: 0.999999984299\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n",
      "Activación: 0.999999977637\tActivación: 0.999999977637\tActivación: 0.999999977637\t\n",
      "-----------------------------\n",
      "Activación: 0.999999984299\tActivación: 0.999999984299\tActivación: 0.999999984299\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.999999966457\tActivación: 0.999999966457\tActivación: 0.999999966457\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.999999985411\tActivación: 0.999999985411\tActivación: 0.999999985411\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.999999966457\tActivación: 0.999999966457\tActivación: 0.999999966457\t\n",
      "-----------------------------\n",
      "Activación: 0.9999999732\tActivación: 0.9999999732\tActivación: 0.9999999732\t\n",
      "-----------------------------\n",
      "Activación: 0.999999982778\tActivación: 0.999999982778\tActivación: 0.999999982778\t\n",
      "-----------------------------\n",
      "Activación: 0.999999955771\tActivación: 0.999999955771\tActivación: 0.999999955771\t\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#transform target to one hot vector\n",
    "nn = RedNeuronal(4, [32, 16], 3)\n",
    "nn.train(X_train, y_train, 1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17290374944691376189\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 174057062\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17761320190815713119\n",
      "physical_device_desc: \"device: 0, name: GeForce 920M, pci bus id: 0000:04:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.2142 - acc: 0.4667\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1933 - acc: 0.6133\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1596 - acc: 0.6733\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1331 - acc: 0.7467\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1135 - acc: 0.7933\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1004 - acc: 0.8333\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0895 - acc: 0.8867\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0793 - acc: 0.8933\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0726 - acc: 0.9133\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0642 - acc: 0.8933\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0607 - acc: 0.9000\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0549 - acc: 0.8933\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0492 - acc: 0.9467\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0439 - acc: 0.9533\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0419 - acc: 0.9467\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0381 - acc: 0.9600\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0325 - acc: 0.9600\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0313 - acc: 0.9667\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0302 - acc: 0.9533\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0284 - acc: 0.9600\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0274 - acc: 0.9600\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0255 - acc: 0.9600\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0229 - acc: 0.9600\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0240 - acc: 0.9733\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0222 - acc: 0.9533\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0206 - acc: 0.9733\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0197 - acc: 0.9867\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0204 - acc: 0.9600\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0169 - acc: 0.9800\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0171 - acc: 0.9733\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0178 - acc: 0.9600\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0186 - acc: 0.9667\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0171 - acc: 0.9600\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0162 - acc: 0.9800\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0170 - acc: 0.9667\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0168 - acc: 0.9733\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0162 - acc: 0.9733\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0165 - acc: 0.9667\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0147 - acc: 0.9733\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0164 - acc: 0.9733\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0160 - acc: 0.9667\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0157 - acc: 0.9667\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0146 - acc: 0.9733\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0147 - acc: 0.9667\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0156 - acc: 0.9800\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0152 - acc: 0.9733\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0145 - acc: 0.9733 ETA: 0s - loss: 0.0073 - acc\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0140 - acc: 0.9667\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0146 - acc: 0.9667\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0148 - acc: 0.9733\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0147 - acc: 0.9667\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0144 - acc: 0.9733\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0128 - acc: 0.9733\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0139 - acc: 0.9800\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0137 - acc: 0.9733\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0132 - acc: 0.9667\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0137 - acc: 0.9733\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0125 - acc: 0.9800\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0140 - acc: 0.9733\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0138 - acc: 0.9667\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0135 - acc: 0.9733\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0131 - acc: 0.9800\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0123 - acc: 0.9667\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0133 - acc: 0.9667\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0127 - acc: 0.9733\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0134 - acc: 0.9733\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0123 - acc: 0.9800\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0126 - acc: 0.9733\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0098 - acc: 0.9867\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0134 - acc: 0.9667\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0121 - acc: 0.9800\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0125 - acc: 0.9800\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0119 - acc: 0.9733\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0119 - acc: 0.9800\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0121 - acc: 0.9733\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0126 - acc: 0.9733\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0126 - acc: 0.9733\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0127 - acc: 0.9800\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0129 - acc: 0.9800\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0115 - acc: 0.9800\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0124 - acc: 0.9733\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0125 - acc: 0.9667\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0124 - acc: 0.9867\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0115 - acc: 0.9800\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0107 - acc: 0.9800\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0127 - acc: 0.9733\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0111 - acc: 0.9800\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0115 - acc: 0.9733\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0120 - acc: 0.9733\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0113 - acc: 0.9733\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0100 - acc: 0.9867\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0128 - acc: 0.9800\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0123 - acc: 0.9667\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0093 - acc: 0.9867\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0127 - acc: 0.9667\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0108 - acc: 0.9733\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0125 - acc: 0.9733\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0106 - acc: 0.9800\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0121 - acc: 0.9733\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0118 - acc: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de6eb80a20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_onehot, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
