{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #se importa numpy como np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    def __init__(self, num_entrada, tam_capas, num_salida):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada))\n",
    "        for i in range(1, len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i-1], self.capas[i-1].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, self.capas[len(self.capas)-1].tam_capa))\n",
    "        \n",
    "    def train(self, entrada, output, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        for ciclo in range(ciclos):\n",
    "            for vector in entrada:\n",
    "                self.capas[0].set_capa(vector) #Poblar capa inicial\n",
    "                # Feed forward\n",
    "                for i in range(1, len(self.capas)):\n",
    "                    self.capas[i].feed_forward(self.capas[i-1])\n",
    "                \n",
    "                self.capas[len(self.capas)-1].mostrar_capa()\n",
    "                \n",
    "                # evaluar error\n",
    "\n",
    "                # Back propagation (actualizar pesos de las neuronas)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior)] * tam_capa # lista de neuronas de la capa\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return [neurona.activacion for neurona in self.neuronas]\n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        for neurona in self.neuronas:\n",
    "            activaciones = capa_anterior.get_activaciones()\n",
    "            pesos = neurona.pesos\n",
    "            result = self.sigmoid(np.dot(pesos, activaciones) + self.bias)          \n",
    "            neurona.set_activacion(result)\n",
    "            \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, valores):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(valores[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        \n",
    "    def set_activacion(self, valor):\n",
    "        self.activacion = valor\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.97878743131\tActivación: 0.97878743131\tActivación: 0.97878743131\t\n",
      "-----------------------------\n",
      "Activación: 0.960662602894\tActivación: 0.960662602894\tActivación: 0.960662602894\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.906928594127\tActivación: 0.906928594127\tActivación: 0.906928594127\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.906928594127\tActivación: 0.906928594127\tActivación: 0.906928594127\t\n",
      "-----------------------------\n",
      "Activación: 0.906928594127\tActivación: 0.906928594127\tActivación: 0.906928594127\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.97878743131\tActivación: 0.97878743131\tActivación: 0.97878743131\t\n",
      "-----------------------------\n",
      "Activación: 0.97878743131\tActivación: 0.97878743131\tActivación: 0.97878743131\t\n",
      "-----------------------------\n",
      "Activación: 0.960662602894\tActivación: 0.960662602894\tActivación: 0.960662602894\t\n",
      "-----------------------------\n",
      "Activación: 0.960662602894\tActivación: 0.960662602894\tActivación: 0.960662602894\t\n",
      "-----------------------------\n",
      "Activación: 0.960662602894\tActivación: 0.960662602894\tActivación: 0.960662602894\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.97878743131\tActivación: 0.97878743131\tActivación: 0.97878743131\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.990047254746\tActivación: 0.990047254746\tActivación: 0.990047254746\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.97878743131\tActivación: 0.97878743131\tActivación: 0.97878743131\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.97878743131\tActivación: 0.97878743131\tActivación: 0.97878743131\t\n",
      "-----------------------------\n",
      "Activación: 0.906928594127\tActivación: 0.906928594127\tActivación: 0.906928594127\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.906928594127\tActivación: 0.906928594127\tActivación: 0.906928594127\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.906928594127\tActivación: 0.906928594127\tActivación: 0.906928594127\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.960662602894\tActivación: 0.960662602894\tActivación: 0.960662602894\t\n",
      "-----------------------------\n",
      "Activación: 0.960662602894\tActivación: 0.960662602894\tActivación: 0.960662602894\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.995923480261\tActivación: 0.995923480261\tActivación: 0.995923480261\t\n",
      "-----------------------------\n",
      "Activación: 0.97878743131\tActivación: 0.97878743131\tActivación: 0.97878743131\t\n",
      "-----------------------------\n",
      "Activación: 0.960662602894\tActivación: 0.960662602894\tActivación: 0.960662602894\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.935963209248\tActivación: 0.935963209248\tActivación: 0.935963209248\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999999442141\tActivación: 0.999999442141\tActivación: 0.999999442141\t\n",
      "-----------------------------\n",
      "Activación: 0.999947603473\tActivación: 0.999947603473\tActivación: 0.999947603473\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999947603473\tActivación: 0.999947603473\tActivación: 0.999947603473\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999947603473\tActivación: 0.999947603473\tActivación: 0.999947603473\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999947603473\tActivación: 0.999947603473\tActivación: 0.999947603473\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999981735749\tActivación: 0.999981735749\tActivación: 0.999981735749\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999992948665\tActivación: 0.999992948665\tActivación: 0.999992948665\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999999607878\tActivación: 0.999999607878\tActivación: 0.999999607878\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999947603473\tActivación: 0.999947603473\tActivación: 0.999947603473\t\n",
      "-----------------------------\n",
      "Activación: 0.999981735749\tActivación: 0.999981735749\tActivación: 0.999981735749\t\n",
      "-----------------------------\n",
      "Activación: 0.999947603473\tActivación: 0.999947603473\tActivación: 0.999947603473\t\n",
      "-----------------------------\n",
      "Activación: 0.999992948665\tActivación: 0.999992948665\tActivación: 0.999992948665\t\n",
      "-----------------------------\n",
      "Activación: 0.999999442141\tActivación: 0.999999442141\tActivación: 0.999999442141\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999999442141\tActivación: 0.999999442141\tActivación: 0.999999442141\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999992948665\tActivación: 0.999992948665\tActivación: 0.999992948665\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999992948665\tActivación: 0.999992948665\tActivación: 0.999992948665\t\n",
      "-----------------------------\n",
      "Activación: 0.999947603473\tActivación: 0.999947603473\tActivación: 0.999947603473\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999992948665\tActivación: 0.999992948665\tActivación: 0.999992948665\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999981735749\tActivación: 0.999981735749\tActivación: 0.999981735749\t\n",
      "-----------------------------\n",
      "Activación: 0.999996914484\tActivación: 0.999996914484\tActivación: 0.999996914484\t\n",
      "-----------------------------\n",
      "Activación: 0.999999854204\tActivación: 0.999999854204\tActivación: 0.999999854204\t\n",
      "-----------------------------\n",
      "Activación: 0.999999757034\tActivación: 0.999999757034\tActivación: 0.999999757034\t\n",
      "-----------------------------\n",
      "Activación: 0.999999815207\tActivación: 0.999999815207\tActivación: 0.999999815207\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999830711\tActivación: 0.999999830711\tActivación: 0.999999830711\t\n",
      "-----------------------------\n",
      "Activación: 0.999999815207\tActivación: 0.999999815207\tActivación: 0.999999815207\t\n",
      "-----------------------------\n",
      "Activación: 0.999999607878\tActivación: 0.999999607878\tActivación: 0.999999607878\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999854204\tActivación: 0.999999854204\tActivación: 0.999999854204\t\n",
      "-----------------------------\n",
      "Activación: 0.999999792201\tActivación: 0.999999792201\tActivación: 0.999999792201\t\n",
      "-----------------------------\n",
      "Activación: 0.999999757034\tActivación: 0.999999757034\tActivación: 0.999999757034\t\n",
      "-----------------------------\n",
      "Activación: 0.999999815207\tActivación: 0.999999815207\tActivación: 0.999999815207\t\n",
      "-----------------------------\n",
      "Activación: 0.999999792201\tActivación: 0.999999792201\tActivación: 0.999999792201\t\n",
      "-----------------------------\n",
      "Activación: 0.999999848886\tActivación: 0.999999848886\tActivación: 0.999999848886\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999830711\tActivación: 0.999999830711\tActivación: 0.999999830711\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999792201\tActivación: 0.999999792201\tActivación: 0.999999792201\t\n",
      "-----------------------------\n",
      "Activación: 0.999999792201\tActivación: 0.999999792201\tActivación: 0.999999792201\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999815207\tActivación: 0.999999815207\tActivación: 0.999999815207\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999815207\tActivación: 0.999999815207\tActivación: 0.999999815207\t\n",
      "-----------------------------\n",
      "Activación: 0.999999442141\tActivación: 0.999999442141\tActivación: 0.999999442141\t\n",
      "-----------------------------\n",
      "Activación: 0.999999757034\tActivación: 0.999999757034\tActivación: 0.999999757034\t\n",
      "-----------------------------\n",
      "Activación: 0.999999792201\tActivación: 0.999999792201\tActivación: 0.999999792201\t\n",
      "-----------------------------\n",
      "Activación: 0.999999830711\tActivación: 0.999999830711\tActivación: 0.999999830711\t\n",
      "-----------------------------\n",
      "Activación: 0.999999124383\tActivación: 0.999999124383\tActivación: 0.999999124383\t\n",
      "-----------------------------\n",
      "Activación: 0.999998458125\tActivación: 0.999998458125\tActivación: 0.999998458125\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999848886\tActivación: 0.999999848886\tActivación: 0.999999848886\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n",
      "Activación: 0.999999815207\tActivación: 0.999999815207\tActivación: 0.999999815207\t\n",
      "-----------------------------\n",
      "Activación: 0.999999848886\tActivación: 0.999999848886\tActivación: 0.999999848886\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999757034\tActivación: 0.999999757034\tActivación: 0.999999757034\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999854204\tActivación: 0.999999854204\tActivación: 0.999999854204\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999757034\tActivación: 0.999999757034\tActivación: 0.999999757034\t\n",
      "-----------------------------\n",
      "Activación: 0.999999792201\tActivación: 0.999999792201\tActivación: 0.999999792201\t\n",
      "-----------------------------\n",
      "Activación: 0.999999841396\tActivación: 0.999999841396\tActivación: 0.999999841396\t\n",
      "-----------------------------\n",
      "Activación: 0.999999701159\tActivación: 0.999999701159\tActivación: 0.999999701159\t\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#transform target to one hot vector\n",
    "nn = RedNeuronal(4, [32, 16], 3)\n",
    "nn.train(X_train, y_train, 1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9.00681170e-01   1.03205722e+00  -1.34127240e+00  -1.31297673e+00]\n",
      " [ -1.14301691e+00  -1.24957601e-01  -1.34127240e+00  -1.31297673e+00]\n",
      " [ -1.38535265e+00   3.37848329e-01  -1.39813811e+00  -1.31297673e+00]\n",
      " [ -1.50652052e+00   1.06445364e-01  -1.28440670e+00  -1.31297673e+00]\n",
      " [ -1.02184904e+00   1.26346019e+00  -1.34127240e+00  -1.31297673e+00]\n",
      " [ -5.37177559e-01   1.95766909e+00  -1.17067529e+00  -1.05003079e+00]\n",
      " [ -1.50652052e+00   8.00654259e-01  -1.34127240e+00  -1.18150376e+00]\n",
      " [ -1.02184904e+00   8.00654259e-01  -1.28440670e+00  -1.31297673e+00]\n",
      " [ -1.74885626e+00  -3.56360566e-01  -1.34127240e+00  -1.31297673e+00]\n",
      " [ -1.14301691e+00   1.06445364e-01  -1.28440670e+00  -1.44444970e+00]\n",
      " [ -5.37177559e-01   1.49486315e+00  -1.28440670e+00  -1.31297673e+00]\n",
      " [ -1.26418478e+00   8.00654259e-01  -1.22754100e+00  -1.31297673e+00]\n",
      " [ -1.26418478e+00  -1.24957601e-01  -1.34127240e+00  -1.44444970e+00]\n",
      " [ -1.87002413e+00  -1.24957601e-01  -1.51186952e+00  -1.44444970e+00]\n",
      " [ -5.25060772e-02   2.18907205e+00  -1.45500381e+00  -1.31297673e+00]\n",
      " [ -1.73673948e-01   3.11468391e+00  -1.28440670e+00  -1.05003079e+00]\n",
      " [ -5.37177559e-01   1.95766909e+00  -1.39813811e+00  -1.05003079e+00]\n",
      " [ -9.00681170e-01   1.03205722e+00  -1.34127240e+00  -1.18150376e+00]\n",
      " [ -1.73673948e-01   1.72626612e+00  -1.17067529e+00  -1.18150376e+00]\n",
      " [ -9.00681170e-01   1.72626612e+00  -1.28440670e+00  -1.18150376e+00]\n",
      " [ -5.37177559e-01   8.00654259e-01  -1.17067529e+00  -1.31297673e+00]\n",
      " [ -9.00681170e-01   1.49486315e+00  -1.28440670e+00  -1.05003079e+00]\n",
      " [ -1.50652052e+00   1.26346019e+00  -1.56873522e+00  -1.31297673e+00]\n",
      " [ -9.00681170e-01   5.69251294e-01  -1.17067529e+00  -9.18557817e-01]\n",
      " [ -1.26418478e+00   8.00654259e-01  -1.05694388e+00  -1.31297673e+00]\n",
      " [ -1.02184904e+00  -1.24957601e-01  -1.22754100e+00  -1.31297673e+00]\n",
      " [ -1.02184904e+00   8.00654259e-01  -1.22754100e+00  -1.05003079e+00]\n",
      " [ -7.79513300e-01   1.03205722e+00  -1.28440670e+00  -1.31297673e+00]\n",
      " [ -7.79513300e-01   8.00654259e-01  -1.34127240e+00  -1.31297673e+00]\n",
      " [ -1.38535265e+00   3.37848329e-01  -1.22754100e+00  -1.31297673e+00]\n",
      " [ -1.26418478e+00   1.06445364e-01  -1.22754100e+00  -1.31297673e+00]\n",
      " [ -5.37177559e-01   8.00654259e-01  -1.28440670e+00  -1.05003079e+00]\n",
      " [ -7.79513300e-01   2.42047502e+00  -1.28440670e+00  -1.44444970e+00]\n",
      " [ -4.16009689e-01   2.65187798e+00  -1.34127240e+00  -1.31297673e+00]\n",
      " [ -1.14301691e+00   1.06445364e-01  -1.28440670e+00  -1.44444970e+00]\n",
      " [ -1.02184904e+00   3.37848329e-01  -1.45500381e+00  -1.31297673e+00]\n",
      " [ -4.16009689e-01   1.03205722e+00  -1.39813811e+00  -1.31297673e+00]\n",
      " [ -1.14301691e+00   1.06445364e-01  -1.28440670e+00  -1.44444970e+00]\n",
      " [ -1.74885626e+00  -1.24957601e-01  -1.39813811e+00  -1.31297673e+00]\n",
      " [ -9.00681170e-01   8.00654259e-01  -1.28440670e+00  -1.31297673e+00]\n",
      " [ -1.02184904e+00   1.03205722e+00  -1.39813811e+00  -1.18150376e+00]\n",
      " [ -1.62768839e+00  -1.74477836e+00  -1.39813811e+00  -1.18150376e+00]\n",
      " [ -1.74885626e+00   3.37848329e-01  -1.39813811e+00  -1.31297673e+00]\n",
      " [ -1.02184904e+00   1.03205722e+00  -1.22754100e+00  -7.87084847e-01]\n",
      " [ -9.00681170e-01   1.72626612e+00  -1.05694388e+00  -1.05003079e+00]\n",
      " [ -1.26418478e+00  -1.24957601e-01  -1.34127240e+00  -1.18150376e+00]\n",
      " [ -9.00681170e-01   1.72626612e+00  -1.22754100e+00  -1.31297673e+00]\n",
      " [ -1.50652052e+00   3.37848329e-01  -1.34127240e+00  -1.31297673e+00]\n",
      " [ -6.58345429e-01   1.49486315e+00  -1.28440670e+00  -1.31297673e+00]\n",
      " [ -1.02184904e+00   5.69251294e-01  -1.34127240e+00  -1.31297673e+00]\n",
      " [  1.40150837e+00   3.37848329e-01   5.35295827e-01   2.64698913e-01]\n",
      " [  6.74501145e-01   3.37848329e-01   4.21564419e-01   3.96171883e-01]\n",
      " [  1.28034050e+00   1.06445364e-01   6.49027235e-01   3.96171883e-01]\n",
      " [ -4.16009689e-01  -1.74477836e+00   1.37235899e-01   1.33225943e-01]\n",
      " [  7.95669016e-01  -5.87763531e-01   4.78430123e-01   3.96171883e-01]\n",
      " [ -1.73673948e-01  -5.87763531e-01   4.21564419e-01   1.33225943e-01]\n",
      " [  5.53333275e-01   5.69251294e-01   5.35295827e-01   5.27644853e-01]\n",
      " [ -1.14301691e+00  -1.51337539e+00  -2.60824029e-01  -2.61192967e-01]\n",
      " [  9.16836886e-01  -3.56360566e-01   4.78430123e-01   1.33225943e-01]\n",
      " [ -7.79513300e-01  -8.19166497e-01   8.03701950e-02   2.64698913e-01]\n",
      " [ -1.02184904e+00  -2.43898725e+00  -1.47092621e-01  -2.61192967e-01]\n",
      " [  6.86617933e-02  -1.24957601e-01   2.50967307e-01   3.96171883e-01]\n",
      " [  1.89829664e-01  -1.97618132e+00   1.37235899e-01  -2.61192967e-01]\n",
      " [  3.10997534e-01  -3.56360566e-01   5.35295827e-01   2.64698913e-01]\n",
      " [ -2.94841818e-01  -3.56360566e-01  -9.02269170e-02   1.33225943e-01]\n",
      " [  1.03800476e+00   1.06445364e-01   3.64698715e-01   2.64698913e-01]\n",
      " [ -2.94841818e-01  -1.24957601e-01   4.21564419e-01   3.96171883e-01]\n",
      " [ -5.25060772e-02  -8.19166497e-01   1.94101603e-01  -2.61192967e-01]\n",
      " [  4.32165405e-01  -1.97618132e+00   4.21564419e-01   3.96171883e-01]\n",
      " [ -2.94841818e-01  -1.28197243e+00   8.03701950e-02  -1.29719997e-01]\n",
      " [  6.86617933e-02   3.37848329e-01   5.92161531e-01   7.90590793e-01]\n",
      " [  3.10997534e-01  -5.87763531e-01   1.37235899e-01   1.33225943e-01]\n",
      " [  5.53333275e-01  -1.28197243e+00   6.49027235e-01   3.96171883e-01]\n",
      " [  3.10997534e-01  -5.87763531e-01   5.35295827e-01   1.75297293e-03]\n",
      " [  6.74501145e-01  -3.56360566e-01   3.07833011e-01   1.33225943e-01]\n",
      " [  9.16836886e-01  -1.24957601e-01   3.64698715e-01   2.64698913e-01]\n",
      " [  1.15917263e+00  -5.87763531e-01   5.92161531e-01   2.64698913e-01]\n",
      " [  1.03800476e+00  -1.24957601e-01   7.05892939e-01   6.59117823e-01]\n",
      " [  1.89829664e-01  -3.56360566e-01   4.21564419e-01   3.96171883e-01]\n",
      " [ -1.73673948e-01  -1.05056946e+00  -1.47092621e-01  -2.61192967e-01]\n",
      " [ -4.16009689e-01  -1.51337539e+00   2.35044910e-02  -1.29719997e-01]\n",
      " [ -4.16009689e-01  -1.51337539e+00  -3.33612130e-02  -2.61192967e-01]\n",
      " [ -5.25060772e-02  -8.19166497e-01   8.03701950e-02   1.75297293e-03]\n",
      " [  1.89829664e-01  -8.19166497e-01   7.62758643e-01   5.27644853e-01]\n",
      " [ -5.37177559e-01  -1.24957601e-01   4.21564419e-01   3.96171883e-01]\n",
      " [  1.89829664e-01   8.00654259e-01   4.21564419e-01   5.27644853e-01]\n",
      " [  1.03800476e+00   1.06445364e-01   5.35295827e-01   3.96171883e-01]\n",
      " [  5.53333275e-01  -1.74477836e+00   3.64698715e-01   1.33225943e-01]\n",
      " [ -2.94841818e-01  -1.24957601e-01   1.94101603e-01   1.33225943e-01]\n",
      " [ -4.16009689e-01  -1.28197243e+00   1.37235899e-01   1.33225943e-01]\n",
      " [ -4.16009689e-01  -1.05056946e+00   3.64698715e-01   1.75297293e-03]\n",
      " [  3.10997534e-01  -1.24957601e-01   4.78430123e-01   2.64698913e-01]\n",
      " [ -5.25060772e-02  -1.05056946e+00   1.37235899e-01   1.75297293e-03]\n",
      " [ -1.02184904e+00  -1.74477836e+00  -2.60824029e-01  -2.61192967e-01]\n",
      " [ -2.94841818e-01  -8.19166497e-01   2.50967307e-01   1.33225943e-01]\n",
      " [ -1.73673948e-01  -1.24957601e-01   2.50967307e-01   1.75297293e-03]\n",
      " [ -1.73673948e-01  -3.56360566e-01   2.50967307e-01   1.33225943e-01]\n",
      " [  4.32165405e-01  -3.56360566e-01   3.07833011e-01   1.33225943e-01]\n",
      " [ -9.00681170e-01  -1.28197243e+00  -4.31421141e-01  -1.29719997e-01]\n",
      " [ -1.73673948e-01  -5.87763531e-01   1.94101603e-01   1.33225943e-01]\n",
      " [  5.53333275e-01   5.69251294e-01   1.27454998e+00   1.71090158e+00]\n",
      " [ -5.25060772e-02  -8.19166497e-01   7.62758643e-01   9.22063763e-01]\n",
      " [  1.52267624e+00  -1.24957601e-01   1.21768427e+00   1.18500970e+00]\n",
      " [  5.53333275e-01  -3.56360566e-01   1.04708716e+00   7.90590793e-01]\n",
      " [  7.95669016e-01  -1.24957601e-01   1.16081857e+00   1.31648267e+00]\n",
      " [  2.12851559e+00  -1.24957601e-01   1.61574420e+00   1.18500970e+00]\n",
      " [ -1.14301691e+00  -1.28197243e+00   4.21564419e-01   6.59117823e-01]\n",
      " [  1.76501198e+00  -3.56360566e-01   1.44514709e+00   7.90590793e-01]\n",
      " [  1.03800476e+00  -1.28197243e+00   1.16081857e+00   7.90590793e-01]\n",
      " [  1.64384411e+00   1.26346019e+00   1.33141568e+00   1.71090158e+00]\n",
      " [  7.95669016e-01   3.37848329e-01   7.62758643e-01   1.05353673e+00]\n",
      " [  6.74501145e-01  -8.19166497e-01   8.76490051e-01   9.22063763e-01]\n",
      " [  1.15917263e+00  -1.24957601e-01   9.90221459e-01   1.18500970e+00]\n",
      " [ -1.73673948e-01  -1.28197243e+00   7.05892939e-01   1.05353673e+00]\n",
      " [ -5.25060772e-02  -5.87763531e-01   7.62758643e-01   1.57942861e+00]\n",
      " [  6.74501145e-01   3.37848329e-01   8.76490051e-01   1.44795564e+00]\n",
      " [  7.95669016e-01  -1.24957601e-01   9.90221459e-01   7.90590793e-01]\n",
      " [  2.24968346e+00   1.72626612e+00   1.67260991e+00   1.31648267e+00]\n",
      " [  2.24968346e+00  -1.05056946e+00   1.78634131e+00   1.44795564e+00]\n",
      " [  1.89829664e-01  -1.97618132e+00   7.05892939e-01   3.96171883e-01]\n",
      " [  1.28034050e+00   3.37848329e-01   1.10395287e+00   1.44795564e+00]\n",
      " [ -2.94841818e-01  -5.87763531e-01   6.49027235e-01   1.05353673e+00]\n",
      " [  2.24968346e+00  -5.87763531e-01   1.67260991e+00   1.05353673e+00]\n",
      " [  5.53333275e-01  -8.19166497e-01   6.49027235e-01   7.90590793e-01]\n",
      " [  1.03800476e+00   5.69251294e-01   1.10395287e+00   1.18500970e+00]\n",
      " [  1.64384411e+00   3.37848329e-01   1.27454998e+00   7.90590793e-01]\n",
      " [  4.32165405e-01  -5.87763531e-01   5.92161531e-01   7.90590793e-01]\n",
      " [  3.10997534e-01  -1.24957601e-01   6.49027235e-01   7.90590793e-01]\n",
      " [  6.74501145e-01  -5.87763531e-01   1.04708716e+00   1.18500970e+00]\n",
      " [  1.64384411e+00  -1.24957601e-01   1.16081857e+00   5.27644853e-01]\n",
      " [  1.88617985e+00  -5.87763531e-01   1.33141568e+00   9.22063763e-01]\n",
      " [  2.49201920e+00   1.72626612e+00   1.50201279e+00   1.05353673e+00]\n",
      " [  6.74501145e-01  -5.87763531e-01   1.04708716e+00   1.31648267e+00]\n",
      " [  5.53333275e-01  -5.87763531e-01   7.62758643e-01   3.96171883e-01]\n",
      " [  3.10997534e-01  -1.05056946e+00   1.04708716e+00   2.64698913e-01]\n",
      " [  2.24968346e+00  -1.24957601e-01   1.33141568e+00   1.44795564e+00]\n",
      " [  5.53333275e-01   8.00654259e-01   1.04708716e+00   1.57942861e+00]\n",
      " [  6.74501145e-01   1.06445364e-01   9.90221459e-01   7.90590793e-01]\n",
      " [  1.89829664e-01  -1.24957601e-01   5.92161531e-01   7.90590793e-01]\n",
      " [  1.28034050e+00   1.06445364e-01   9.33355755e-01   1.18500970e+00]\n",
      " [  1.03800476e+00   1.06445364e-01   1.04708716e+00   1.57942861e+00]\n",
      " [  1.28034050e+00   1.06445364e-01   7.62758643e-01   1.44795564e+00]\n",
      " [ -5.25060772e-02  -8.19166497e-01   7.62758643e-01   9.22063763e-01]\n",
      " [  1.15917263e+00   3.37848329e-01   1.21768427e+00   1.44795564e+00]\n",
      " [  1.03800476e+00   5.69251294e-01   1.10395287e+00   1.71090158e+00]\n",
      " [  1.03800476e+00  -1.24957601e-01   8.19624347e-01   1.44795564e+00]\n",
      " [  5.53333275e-01  -1.28197243e+00   7.05892939e-01   9.22063763e-01]\n",
      " [  7.95669016e-01  -1.24957601e-01   8.19624347e-01   1.05353673e+00]\n",
      " [  4.32165405e-01   8.00654259e-01   9.33355755e-01   1.44795564e+00]\n",
      " [  6.86617933e-02  -1.24957601e-01   7.62758643e-01   7.90590793e-01]] [[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8253083999358165994\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 0.9867\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.9867\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.9867\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0113 - acc: 0.9800A: 0s - loss: 0.0187 - acc: 0.9\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 0.9800- ETA: 0s - loss: 0.0067 - acc: 0.9773 \n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 0.9800\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 0.9800\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0111 - acc: 0.9800\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 0.9800\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 0.9800\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 0.9800\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0102 - acc: 0.9800\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.9800\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0102 - acc: 0.9867\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 912us/step - loss: 0.0099 - acc: 0.9800\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 989us/step - loss: 0.0096 - acc: 0.9867\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 929us/step - loss: 0.0092 - acc: 0.9733\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 942us/step - loss: 0.0115 - acc: 0.9733\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 916us/step - loss: 0.0102 - acc: 0.9800\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.9800\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 949us/step - loss: 0.0106 - acc: 0.9733\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 949us/step - loss: 0.0103 - acc: 0.9867\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 929us/step - loss: 0.0100 - acc: 0.9867\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 0.9867\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 963us/step - loss: 0.0102 - acc: 0.9800\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 923us/step - loss: 0.0090 - acc: 0.9800\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.9800\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.9867\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 976us/step - loss: 0.0106 - acc: 0.9667\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 976us/step - loss: 0.0096 - acc: 0.9867\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 963us/step - loss: 0.0095 - acc: 0.9800\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 983us/step - loss: 0.0107 - acc: 0.9867\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 976us/step - loss: 0.0100 - acc: 0.9800\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 996us/step - loss: 0.0099 - acc: 0.9800\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 932us/step - loss: 0.0092 - acc: 0.9867\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 996us/step - loss: 0.0097 - acc: 0.9800\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 942us/step - loss: 0.0102 - acc: 0.9800\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 956us/step - loss: 0.0100 - acc: 0.9867\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 976us/step - loss: 0.0097 - acc: 0.9867\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 969us/step - loss: 0.0114 - acc: 0.9867\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0093 - acc: 0.9867\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 902us/step - loss: 0.0093 - acc: 0.9800\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 996us/step - loss: 0.0096 - acc: 0.9800\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 0.9800\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 929us/step - loss: 0.0088 - acc: 0.9867\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 969us/step - loss: 0.0089 - acc: 0.9867\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0098 - acc: 0.9800\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 902us/step - loss: 0.0090 - acc: 0.9933\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 956us/step - loss: 0.0078 - acc: 0.9933\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.9800\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 929us/step - loss: 0.0088 - acc: 0.9933\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 989us/step - loss: 0.0096 - acc: 0.9867\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0093 - acc: 0.9800\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 963us/step - loss: 0.0106 - acc: 0.9800\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 937us/step - loss: 0.0098 - acc: 0.9733\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 983us/step - loss: 0.0088 - acc: 0.9867\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0098 - acc: 0.9733\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 936us/step - loss: 0.0101 - acc: 0.9867\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 0.9867\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 0.9800\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 876us/step - loss: 0.0098 - acc: 0.9867\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0099 - acc: 0.9867\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 956us/step - loss: 0.0096 - acc: 0.9800\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 942us/step - loss: 0.0097 - acc: 0.9800\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 996us/step - loss: 0.0093 - acc: 0.9867\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 956us/step - loss: 0.0085 - acc: 0.9800\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.9733\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 980us/step - loss: 0.0091 - acc: 0.9867\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 936us/step - loss: 0.0081 - acc: 0.9800\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.9733\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 976us/step - loss: 0.0086 - acc: 0.9800\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 956us/step - loss: 0.0100 - acc: 0.9733\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 0.9800\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 936us/step - loss: 0.0109 - acc: 0.9867\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0084 - acc: 0.9867\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 969us/step - loss: 0.0109 - acc: 0.9867\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 956us/step - loss: 0.0100 - acc: 0.9800\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 916us/step - loss: 0.0097 - acc: 0.9800\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 983us/step - loss: 0.0099 - acc: 0.9800\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 963us/step - loss: 0.0098 - acc: 0.9800\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 949us/step - loss: 0.0085 - acc: 0.9733\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 963us/step - loss: 0.0106 - acc: 0.9800\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.0108 - acc: 0.9735   - 0s 909us/step - loss: 0.0095 - acc: 0.9800\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 922us/step - loss: 0.0084 - acc: 0.9867\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0080 - acc: 0.9867\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 896us/step - loss: 0.0098 - acc: 0.9800\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 0.9800\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0087 - acc: 0.9800\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 882us/step - loss: 0.0105 - acc: 0.9733\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0093 - acc: 0.9867\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0085 - acc: 0.9867\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 943us/step - loss: 0.0094 - acc: 0.9867\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0093 - acc: 0.9800\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 969us/step - loss: 0.0086 - acc: 0.9800\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 929us/step - loss: 0.0085 - acc: 0.9867\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 904us/step - loss: 0.0086 - acc: 0.9867\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 882us/step - loss: 0.0102 - acc: 0.9800\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 876us/step - loss: 0.0098 - acc: 0.9733\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0086 - acc: 0.9867\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 922us/step - loss: 0.0098 - acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b5982ba90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_onehot, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
