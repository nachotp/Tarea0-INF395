{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducci칩n a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe Gonz치lez - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #se importa numpy como np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    def __init__(self, num_entrada, tam_capas, num_salida, act_inter, cost_func):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, act_inter))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], act_inter, self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        if cost_func == \"crossentropy\":\n",
    "            self.cost = self.crossentropy\n",
    "            self.cost_derivative = self.crossentropy_derivative\n",
    "        elif cost_func == \"mse\":\n",
    "            self.cost = self.mse\n",
    "            self.cost_derivative = self.mse_derivative\n",
    "        \n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+1e-9)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado):\n",
    "        return (esperado-1/prediccion+(1+esperado)*(1/(1-prediccion)))  \n",
    "    \n",
    "    def mse(self, prediccion, esperado):\n",
    "        return np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def mse_derivative(self, prediccion, esperado):\n",
    "        return prediccion-esperado\n",
    "    \n",
    "    def train(self, entrada, salida, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            for input_v, output in resultList:\n",
    "                \n",
    "                self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta funci칩n costo)\n",
    "                error_epoch = self.cost(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                ce_derivate = self.cost_derivative(self.capas[last].get_activaciones(), np.transpose(output))\n",
    "                delta = ce_derivate * self.capas[last].squash_derivative(self.capas[last].get_vector_z())\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                self.capas[last].actualizar_pesos(np.outer(self.capas[last-1].get_activaciones(), delta), tasa)\n",
    "                #print(self.capas[last].get_pesos())\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    self.capas[-l].actualizar_pesos(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa)\n",
    "                    \n",
    "            print(\"Error del epoch\",ciclo,\":\", str(np.round(np.mean(errores),7)))\n",
    "        return errores\n",
    "\n",
    "            \n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            # evaluar error final (delta funci칩n costo)\n",
    "            error_epoch = self.crossentropy(self.capas[last].get_activaciones(), output)\n",
    "            errores.append(error_epoch)\n",
    "            print(self.capas[last].get_activaciones(), output)\n",
    "        print(\"Error del test:\", str(np.round(np.mean(errores),7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"tanh\":\n",
    "            self.squash = self.tanh\n",
    "            self.squash_derivative = self.tanh_derivative\n",
    "        elif tipo == \"relu\":\n",
    "            self.squash = self.relu\n",
    "            self.squash_derivative = self.relu_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def tanh_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        x= np.array(x)\n",
    "        return np.maximum(x, 0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        x = np.array(x)\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "        \n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tama침o \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activaci칩n: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 춰Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error del epoch 0 : 0.7099084\n",
      "Error del epoch 1 : 0.7194283\n",
      "Error del epoch 2 : 0.6940541\n",
      "Error del epoch 3 : 0.7020013\n",
      "Error del epoch 4 : 0.7294889\n",
      "Error del epoch 5 : 0.7103572\n",
      "Error del epoch 6 : 0.7224873\n",
      "Error del epoch 7 : 0.721788\n",
      "Error del epoch 8 : 0.7189185\n",
      "Error del epoch 9 : 0.7148753\n",
      "Error del epoch 10 : 0.7168664\n",
      "Error del epoch 11 : 0.6955322\n",
      "Error del epoch 12 : 0.7210869\n",
      "Error del epoch 13 : 0.7115522\n",
      "Error del epoch 14 : 0.7189777\n",
      "Error del epoch 15 : 0.7163099\n",
      "Error del epoch 16 : 0.7185008\n",
      "Error del epoch 17 : 0.7105343\n",
      "Error del epoch 18 : 0.7099925\n",
      "Error del epoch 19 : 0.7242419\n",
      "Error del epoch 20 : 0.7167283\n",
      "Error del epoch 21 : 0.7105941\n",
      "Error del epoch 22 : 0.7080135\n",
      "Error del epoch 23 : 0.7119852\n",
      "Error del epoch 24 : 0.6886212\n",
      "Error del epoch 25 : 0.6998307\n",
      "Error del epoch 26 : 0.6867997\n",
      "Error del epoch 27 : 0.7086529\n",
      "Error del epoch 28 : 0.7064203\n",
      "Error del epoch 29 : 0.694941\n",
      "Error del epoch 30 : 0.6856667\n",
      "Error del epoch 31 : 0.7003869\n",
      "Error del epoch 32 : 0.7054326\n",
      "Error del epoch 33 : 0.6952288\n",
      "Error del epoch 34 : 0.7089917\n",
      "Error del epoch 35 : 0.6946464\n",
      "Error del epoch 36 : 0.689867\n",
      "Error del epoch 37 : 0.6881764\n",
      "Error del epoch 38 : 0.7075613\n",
      "Error del epoch 39 : 0.6917834\n",
      "Error del epoch 40 : 0.6864189\n",
      "Error del epoch 41 : 0.6928249\n",
      "Error del epoch 42 : 0.6863784\n",
      "Error del epoch 43 : 0.6804181\n",
      "Error del epoch 44 : 0.6739696\n",
      "Error del epoch 45 : 0.6552461\n",
      "Error del epoch 46 : 0.6585853\n",
      "Error del epoch 47 : 0.6453406\n",
      "Error del epoch 48 : 0.6116784\n",
      "Error del epoch 49 : 0.6049701\n",
      "Error del epoch 50 : 0.5643386\n",
      "Error del epoch 51 : 0.524109\n",
      "Error del epoch 52 : 0.5225805\n",
      "Error del epoch 53 : 0.4851734\n",
      "Error del epoch 54 : 0.4598841\n",
      "Error del epoch 55 : 0.4267721\n",
      "Error del epoch 56 : 0.4223586\n",
      "Error del epoch 57 : 0.439953\n",
      "Error del epoch 58 : 0.4046455\n",
      "Error del epoch 59 : 0.417042\n",
      "Error del epoch 60 : 0.3750729\n",
      "Error del epoch 61 : 0.4002498\n",
      "Error del epoch 62 : 0.3945842\n",
      "Error del epoch 63 : 0.3836542\n",
      "Error del epoch 64 : 0.374241\n",
      "Error del epoch 65 : 0.3726044\n",
      "Error del epoch 66 : 0.3485581\n",
      "Error del epoch 67 : 0.3394342\n",
      "Error del epoch 68 : 0.3391243\n",
      "Error del epoch 69 : 0.3143889\n",
      "Error del epoch 70 : 0.3189105\n",
      "Error del epoch 71 : 0.3035812\n",
      "Error del epoch 72 : 0.2961681\n",
      "Error del epoch 73 : 0.2710429\n",
      "Error del epoch 74 : 0.2989903\n",
      "Error del epoch 75 : 0.2795319\n",
      "Error del epoch 76 : 0.2593003\n",
      "Error del epoch 77 : 0.2683184\n",
      "Error del epoch 78 : 0.267023\n",
      "Error del epoch 79 : 0.2571096\n",
      "Error del epoch 80 : 0.2381841\n",
      "Error del epoch 81 : 0.2433185\n",
      "Error del epoch 82 : 0.2373075\n",
      "Error del epoch 83 : 0.2137243\n",
      "Error del epoch 84 : 0.219244\n",
      "Error del epoch 85 : 0.2102639\n",
      "Error del epoch 86 : 0.2063319\n",
      "Error del epoch 87 : 0.2064388\n",
      "Error del epoch 88 : 0.1922179\n",
      "Error del epoch 89 : 0.1861952\n",
      "Error del epoch 90 : 0.1549062\n",
      "Error del epoch 91 : 0.1877294\n",
      "Error del epoch 92 : 0.1676943\n",
      "Error del epoch 93 : 0.1605741\n",
      "Error del epoch 94 : 0.1536304\n",
      "Error del epoch 95 : 0.1619248\n",
      "Error del epoch 96 : 0.1600298\n",
      "Error del epoch 97 : 0.1388096\n",
      "Error del epoch 98 : 0.1558704\n",
      "Error del epoch 99 : 0.1370923\n",
      "TESTING\n",
      "[ 0.01424726  0.85988955  0.12586319] [ 0.  1.  0.]\n",
      "[ 0.88668256  0.10852874  0.0047887 ] [ 1.  0.  0.]\n",
      "[ 0.00297682  0.17330158  0.8237216 ] [ 0.  0.  1.]\n",
      "[ 0.01303753  0.80049433  0.18646814] [ 0.  1.  0.]\n",
      "[ 0.01068322  0.67154944  0.31776734] [ 0.  1.  0.]\n",
      "[ 0.89049687  0.1044934   0.00500973] [ 1.  0.  0.]\n",
      "[ 0.02547638  0.9115671   0.06295651] [ 0.  1.  0.]\n",
      "[ 0.00453776  0.26964119  0.72582105] [ 0.  0.  1.]\n",
      "[ 0.01182968  0.75449796  0.23367236] [ 0.  1.  0.]\n",
      "[ 0.01923641  0.91048056  0.07028303] [ 0.  1.  0.]\n",
      "[ 0.0061972   0.37361255  0.62019025] [ 0.  0.  1.]\n",
      "[ 0.91997102  0.07546379  0.00456519] [ 1.  0.  0.]\n",
      "[ 0.91096883  0.08486925  0.00416192] [ 1.  0.  0.]\n",
      "[ 0.92025922  0.07536495  0.00437583] [ 1.  0.  0.]\n",
      "[ 0.92328039  0.07317609  0.00354352] [ 1.  0.  0.]\n",
      "[ 0.0111199   0.68398705  0.30489305] [ 0.  1.  0.]\n",
      "[ 0.0040165   0.23699417  0.75898933] [ 0.  0.  1.]\n",
      "[ 0.02305567  0.91392409  0.06302024] [ 0.  1.  0.]\n",
      "[ 0.01577112  0.88077244  0.10345644] [ 0.  1.  0.]\n",
      "[ 0.00401993  0.23727784  0.75870223] [ 0.  0.  1.]\n",
      "[ 0.92258322  0.07322743  0.00418935] [ 1.  0.  0.]\n",
      "[ 0.00866061  0.52998738  0.461352  ] [ 0.  0.  1.]\n",
      "[ 0.91022227  0.0854384   0.00433933] [ 1.  0.  0.]\n",
      "[ 0.0043178   0.25577445  0.73990775] [ 0.  0.  1.]\n",
      "[ 0.00589353  0.35641179  0.63769468] [ 0.  0.  1.]\n",
      "[ 0.00429862  0.25463063  0.74107075] [ 0.  0.  1.]\n",
      "[ 0.00466833  0.27828939  0.71704228] [ 0.  0.  1.]\n",
      "[ 0.00398075  0.23490686  0.7611124 ] [ 0.  0.  1.]\n",
      "[ 0.91100197  0.0840877   0.00491033] [ 1.  0.  0.]\n",
      "[ 0.9168653   0.07861782  0.00451687] [ 1.  0.  0.]\n",
      "[ 0.93795002  0.05876615  0.00328383] [ 1.  0.  0.]\n",
      "[ 0.8915356   0.10396598  0.00449842] [ 1.  0.  0.]\n",
      "[ 0.01233952  0.77146003  0.21620045] [ 0.  1.  0.]\n",
      "[ 0.9263286   0.06988104  0.00379036] [ 1.  0.  0.]\n",
      "[ 0.92953469  0.06641777  0.00404754] [ 1.  0.  0.]\n",
      "[ 0.00646353  0.39244665  0.60108982] [ 0.  0.  1.]\n",
      "[ 0.01218181  0.75222634  0.23559185] [ 0.  1.  0.]\n",
      "[ 0.91928962  0.07682008  0.0038903 ] [ 1.  0.  0.]\n",
      "[ 0.92838778  0.06809409  0.00351813] [ 1.  0.  0.]\n",
      "[ 0.93056157  0.06627801  0.00316042] [ 1.  0.  0.]\n",
      "[ 0.00754869  0.45953988  0.53291143] [ 0.  0.  1.]\n",
      "[ 0.01283089  0.76738001  0.21978911] [ 0.  1.  0.]\n",
      "[ 0.01070489  0.66713655  0.32215856] [ 0.  1.  0.]\n",
      "[ 0.91082403  0.08525927  0.0039167 ] [ 1.  0.  0.]\n",
      "[ 0.91586825  0.08029481  0.00383694] [ 1.  0.  0.]\n",
      "[ 0.03280334  0.91124844  0.05594822] [ 0.  1.  0.]\n",
      "[ 0.01005329  0.62462647  0.36532024] [ 0.  0.  1.]\n",
      "[ 0.00638987  0.38565227  0.60795786] [ 0.  0.  1.]\n",
      "[ 0.01249872  0.7824805   0.20502078] [ 0.  1.  0.]\n",
      "[ 0.00428725  0.25423318  0.74147958] [ 0.  0.  1.]\n",
      "Error del test: 0.079835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "#transform target to one hot vector\n",
    "nn = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "nn.train(x_train, y_onehot_train, 100, 0.1)\n",
    "print(\"TESTING\")\n",
    "nn.test(x_test, y_onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15141469021641961845\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2282 - acc: 0.3500\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.2169 - acc: 0.4300\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.2097 - acc: 0.3800\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.1985 - acc: 0.6100\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.1829 - acc: 0.6400\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 712us/step - loss: 0.1635 - acc: 0.6800\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.1451 - acc: 0.7000\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.1323 - acc: 0.7300\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.1196 - acc: 0.8200\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.1091 - acc: 0.8200\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.1013 - acc: 0.8800\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0951 - acc: 0.8600\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.0875 - acc: 0.9000\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0812 - acc: 0.9200\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0761 - acc: 0.9100\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0704 - acc: 0.9000\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0633 - acc: 0.9300\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0599 - acc: 0.9300\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0585 - acc: 0.9200\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.0547 - acc: 0.9300\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.0519 - acc: 0.9300\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.0446 - acc: 0.9600\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 707us/step - loss: 0.0471 - acc: 0.9200\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 697us/step - loss: 0.0425 - acc: 0.9500\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0410 - acc: 0.9400\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.0400 - acc: 0.9500\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 707us/step - loss: 0.0380 - acc: 0.9400\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 703us/step - loss: 0.0366 - acc: 0.9600\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.0339 - acc: 0.9500\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0332 - acc: 0.9500\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0321 - acc: 0.9500\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0279 - acc: 0.9700\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 676us/step - loss: 0.0295 - acc: 0.9500\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.0278 - acc: 0.9500\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0278 - acc: 0.9500\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.0257 - acc: 0.9600\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 698us/step - loss: 0.0264 - acc: 0.9400\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0251 - acc: 0.9600\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.0246 - acc: 0.9600\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0235 - acc: 0.9700\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0233 - acc: 0.9500\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 738us/step - loss: 0.0208 - acc: 0.9700\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0225 - acc: 0.9600\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0228 - acc: 0.9600\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0214 - acc: 0.9600\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0220 - acc: 0.9500\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0204 - acc: 0.9600\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0199 - acc: 0.9700\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0196 - acc: 0.9800\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.0199 - acc: 0.9700\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0182 - acc: 0.9700\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0197 - acc: 0.9700\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0193 - acc: 0.9700\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0190 - acc: 0.9800\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 638us/step - loss: 0.0158 - acc: 0.9700\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0189 - acc: 0.9700\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0192 - acc: 0.9600\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.0164 - acc: 0.9700\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.0194 - acc: 0.9500\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.0172 - acc: 0.9700\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.0176 - acc: 0.9600\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0179 - acc: 0.9700\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0174 - acc: 0.9500\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0180 - acc: 0.9600\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0152 - acc: 0.9800\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0172 - acc: 0.9700\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0135 - acc: 0.9800\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0164 - acc: 0.9800\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0149 - acc: 0.9700\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0154 - acc: 0.9600\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 668us/step - loss: 0.0171 - acc: 0.9600\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0164 - acc: 0.9700\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0154 - acc: 0.9700\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0146 - acc: 0.9800\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0169 - acc: 0.9600\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0164 - acc: 0.9600\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0154 - acc: 0.9800\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0161 - acc: 0.9700\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0155 - acc: 0.9700\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 669us/step - loss: 0.0155 - acc: 0.9600\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0150 - acc: 0.9800\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 657us/step - loss: 0.0143 - acc: 0.9800\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0145 - acc: 0.9800\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0148 - acc: 0.9700\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0148 - acc: 0.9700\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0143 - acc: 0.9800\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.0146 - acc: 0.9700\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0124 - acc: 0.9700\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0150 - acc: 0.9800\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0136 - acc: 0.9600\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0146 - acc: 0.9700\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.0122 - acc: 0.9800\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.0142 - acc: 0.9800\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0139 - acc: 0.9800\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0133 - acc: 0.9700\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 627us/step - loss: 0.0149 - acc: 0.9700\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.0144 - acc: 0.9700\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.0120 - acc: 0.9700\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0134 - acc: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c9b44486d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_onehot_train, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
