{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    \n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    # act_inter: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\"\n",
    "    # cost_func: función de costo: \"crossentropy\" / \"mse\" \n",
    "    def __init__(self, num_entrada, tam_capas, num_salida, act_inter, cost_func):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, act_inter))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], act_inter, self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        if cost_func == \"crossentropy\":\n",
    "            self.cost = self.crossentropy\n",
    "            self.cost_derivative = self.crossentropy_derivative\n",
    "        elif cost_func == \"mse\":\n",
    "            self.cost = self.mse\n",
    "            self.cost_derivative = self.mse_derivative\n",
    "        \n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado):\n",
    "        return (esperado-1/prediccion+(1+esperado)*(1/(1-prediccion)))  \n",
    "    \n",
    "    def mse(self, prediccion, esperado):\n",
    "        return np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def mse_derivative(self, prediccion, esperado):\n",
    "        return prediccion-esperado\n",
    "    \n",
    "    # train(entrada, salida, ciclos, tasa):\n",
    "    # entrada: lista de datos de entrada\n",
    "    # salida: lista de vectores de salida tipo onehot\n",
    "    # ciclos: cantidad de epochs\n",
    "    # tasa: tasa de aprendizaje \n",
    "    # retorna loss y accuracy\n",
    "    def train(self, entrada, salida, ciclos, tasa, verbose = False):\n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        \n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            acertados = 0\n",
    "            for input_v, output in resultList:\n",
    "                #Poblar capa inicial\n",
    "                self.capas[0].set_capa(input_v, input_v) \n",
    "                \n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                prediccion = self.capas[last].get_activaciones()\n",
    "                error_epoch = self.cost(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                \n",
    "                if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                ce_derivate = self.cost_derivative(prediccion, np.transpose(output))\n",
    "                delta = ce_derivate * self.capas[last].squash_derivative(self.capas[last].get_vector_z())\n",
    "                \n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                self.capas[last].actualizar_pesos(np.outer(self.capas[last-1].get_activaciones(), delta), tasa)\n",
    "\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    self.capas[-l].actualizar_pesos(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa)\n",
    "                \n",
    "            loss.append(np.mean(errores))\n",
    "            accuracy.append(acertados/len(resultList))\n",
    "            if verbose:\n",
    "                print(\"\\tError del epoch \"+str(ciclo)+\": loss\", str(np.round(np.mean(errores),5)),\"- acc\", acertados/len(resultList))\n",
    "        print(\"Resultado de training set de\", ciclos,\"epochs: loss\", np.mean(errores),\"- acc:\", acertados/len(resultList))\n",
    "        return loss, accuracy\n",
    "\n",
    "    # test(entrada, salida):\n",
    "    # entrada: lista de datos de entrada de test\n",
    "    # salida: Lista de datos esperados de test, en formato onehot vector\n",
    "    # retorna loss y accuracy\n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        acertados = 0\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            \n",
    "            prediccion = self.capas[last].get_activaciones()\n",
    "            \n",
    "            if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                    \n",
    "            # evaluar error final (delta función costo)\n",
    "            error_epoch = self.crossentropy(prediccion, output)\n",
    "            errores.append(error_epoch)\n",
    "        \n",
    "        print(\"Error del test:\", str(np.round(np.mean(errores),7)))\n",
    "        return np.mean(errores), acertados/len(resultList)\n",
    "    \n",
    "    def predict(self, entrada, categorias):\n",
    "        last = self.cant_capas-1\n",
    "        self.capas[0].set_capa(entrada, entrada)\n",
    "        \n",
    "        for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "        \n",
    "        prediccion = self.capas[last].get_activaciones()\n",
    "        \n",
    "        return categorias[np.argmax(prediccion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"arctan\":\n",
    "            self.squash = self.arctan\n",
    "            self.squash_derivative = self.arctan_derivative\n",
    "        elif tipo == \"relu\":\n",
    "            self.squash = self.relu\n",
    "            self.squash_derivative = self.relu_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def arctan(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def arctan_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        x= np.array(x)\n",
    "        return np.maximum(x, 0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        x = np.array(x)\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "        \n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "        \n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "tags = ['setosa', 'versicolor', 'verginica']\n",
    "\n",
    "# Instanciar red neuronal\n",
    "nn = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tError del epoch 0: loss 0.72314 - acc 0.33\n",
      "\tError del epoch 1: loss 0.69996 - acc 0.41\n",
      "\tError del epoch 2: loss 0.72263 - acc 0.28\n",
      "\tError del epoch 3: loss 0.68812 - acc 0.37\n",
      "\tError del epoch 4: loss 0.72664 - acc 0.25\n",
      "\tError del epoch 5: loss 0.68842 - acc 0.38\n",
      "\tError del epoch 6: loss 0.71495 - acc 0.29\n",
      "\tError del epoch 7: loss 0.71306 - acc 0.3\n",
      "\tError del epoch 8: loss 0.70904 - acc 0.3\n",
      "\tError del epoch 9: loss 0.716 - acc 0.28\n",
      "\tError del epoch 10: loss 0.7228 - acc 0.32\n",
      "\tError del epoch 11: loss 0.7131 - acc 0.3\n",
      "\tError del epoch 12: loss 0.71384 - acc 0.29\n",
      "\tError del epoch 13: loss 0.69624 - acc 0.39\n",
      "\tError del epoch 14: loss 0.68315 - acc 0.45\n",
      "\tError del epoch 15: loss 0.70734 - acc 0.32\n",
      "\tError del epoch 16: loss 0.71976 - acc 0.29\n",
      "\tError del epoch 17: loss 0.70253 - acc 0.35\n",
      "\tError del epoch 18: loss 0.70772 - acc 0.38\n",
      "\tError del epoch 19: loss 0.70645 - acc 0.3\n",
      "\tError del epoch 20: loss 0.69515 - acc 0.34\n",
      "\tError del epoch 21: loss 0.67255 - acc 0.43\n",
      "\tError del epoch 22: loss 0.70526 - acc 0.33\n",
      "\tError del epoch 23: loss 0.70652 - acc 0.27\n",
      "\tError del epoch 24: loss 0.69435 - acc 0.37\n",
      "\tError del epoch 25: loss 0.6974 - acc 0.33\n",
      "\tError del epoch 26: loss 0.69638 - acc 0.32\n",
      "\tError del epoch 27: loss 0.70092 - acc 0.39\n",
      "\tError del epoch 28: loss 0.68554 - acc 0.37\n",
      "\tError del epoch 29: loss 0.70612 - acc 0.28\n",
      "\tError del epoch 30: loss 0.67976 - acc 0.4\n",
      "\tError del epoch 31: loss 0.69409 - acc 0.33\n",
      "\tError del epoch 32: loss 0.67239 - acc 0.4\n",
      "\tError del epoch 33: loss 0.68075 - acc 0.37\n",
      "\tError del epoch 34: loss 0.6781 - acc 0.37\n",
      "\tError del epoch 35: loss 0.64509 - acc 0.43\n",
      "\tError del epoch 36: loss 0.63075 - acc 0.43\n",
      "\tError del epoch 37: loss 0.59545 - acc 0.55\n",
      "\tError del epoch 38: loss 0.58609 - acc 0.52\n",
      "\tError del epoch 39: loss 0.54483 - acc 0.59\n",
      "\tError del epoch 40: loss 0.52366 - acc 0.61\n",
      "\tError del epoch 41: loss 0.4962 - acc 0.67\n",
      "\tError del epoch 42: loss 0.48828 - acc 0.61\n",
      "\tError del epoch 43: loss 0.47758 - acc 0.62\n",
      "\tError del epoch 44: loss 0.45189 - acc 0.63\n",
      "\tError del epoch 45: loss 0.4376 - acc 0.62\n",
      "\tError del epoch 46: loss 0.4056 - acc 0.73\n",
      "\tError del epoch 47: loss 0.40145 - acc 0.71\n",
      "\tError del epoch 48: loss 0.41565 - acc 0.65\n",
      "\tError del epoch 49: loss 0.40711 - acc 0.68\n",
      "\tError del epoch 50: loss 0.39442 - acc 0.7\n",
      "\tError del epoch 51: loss 0.41687 - acc 0.62\n",
      "\tError del epoch 52: loss 0.39004 - acc 0.68\n",
      "\tError del epoch 53: loss 0.41641 - acc 0.64\n",
      "\tError del epoch 54: loss 0.37704 - acc 0.7\n",
      "\tError del epoch 55: loss 0.41497 - acc 0.62\n",
      "\tError del epoch 56: loss 0.36426 - acc 0.73\n",
      "\tError del epoch 57: loss 0.40324 - acc 0.67\n",
      "\tError del epoch 58: loss 0.37502 - acc 0.67\n",
      "\tError del epoch 59: loss 0.37852 - acc 0.7\n",
      "\tError del epoch 60: loss 0.38516 - acc 0.65\n",
      "\tError del epoch 61: loss 0.39371 - acc 0.61\n",
      "\tError del epoch 62: loss 0.37152 - acc 0.69\n",
      "\tError del epoch 63: loss 0.37238 - acc 0.69\n",
      "\tError del epoch 64: loss 0.37409 - acc 0.71\n",
      "\tError del epoch 65: loss 0.36615 - acc 0.71\n",
      "\tError del epoch 66: loss 0.36917 - acc 0.71\n",
      "\tError del epoch 67: loss 0.36874 - acc 0.7\n",
      "\tError del epoch 68: loss 0.35004 - acc 0.72\n",
      "\tError del epoch 69: loss 0.3324 - acc 0.74\n",
      "\tError del epoch 70: loss 0.35809 - acc 0.71\n",
      "\tError del epoch 71: loss 0.35056 - acc 0.7\n",
      "\tError del epoch 72: loss 0.33602 - acc 0.75\n",
      "\tError del epoch 73: loss 0.34711 - acc 0.75\n",
      "\tError del epoch 74: loss 0.35202 - acc 0.71\n",
      "\tError del epoch 75: loss 0.33722 - acc 0.75\n",
      "\tError del epoch 76: loss 0.33714 - acc 0.75\n",
      "\tError del epoch 77: loss 0.32459 - acc 0.75\n",
      "\tError del epoch 78: loss 0.34356 - acc 0.78\n",
      "\tError del epoch 79: loss 0.32271 - acc 0.76\n",
      "\tError del epoch 80: loss 0.31976 - acc 0.76\n",
      "\tError del epoch 81: loss 0.31402 - acc 0.78\n",
      "\tError del epoch 82: loss 0.3113 - acc 0.79\n",
      "\tError del epoch 83: loss 0.30339 - acc 0.81\n",
      "\tError del epoch 84: loss 0.30436 - acc 0.82\n",
      "\tError del epoch 85: loss 0.30509 - acc 0.79\n",
      "\tError del epoch 86: loss 0.28254 - acc 0.83\n",
      "\tError del epoch 87: loss 0.29037 - acc 0.81\n",
      "\tError del epoch 88: loss 0.27725 - acc 0.85\n",
      "\tError del epoch 89: loss 0.28233 - acc 0.86\n",
      "\tError del epoch 90: loss 0.26813 - acc 0.84\n",
      "\tError del epoch 91: loss 0.26551 - acc 0.88\n",
      "\tError del epoch 92: loss 0.2596 - acc 0.84\n",
      "\tError del epoch 93: loss 0.24899 - acc 0.87\n",
      "\tError del epoch 94: loss 0.25316 - acc 0.89\n",
      "\tError del epoch 95: loss 0.23621 - acc 0.89\n",
      "\tError del epoch 96: loss 0.23997 - acc 0.9\n",
      "\tError del epoch 97: loss 0.21987 - acc 0.89\n",
      "\tError del epoch 98: loss 0.21223 - acc 0.91\n",
      "\tError del epoch 99: loss 0.219 - acc 0.91\n",
      "Resultado de training set de 100 epochs: loss 0.219001804475 - acc: 0.91\n",
      "\n",
      "TESTING\n",
      "Error del test: 0.1173793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11737933106230444, 0.92)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenar la red\n",
    "loss, accuracy = nn.train(x_train, y_onehot_train, 100, 0.1, verbose = True)\n",
    "\n",
    "#Testear\n",
    "print(\"\\nTESTING\")\n",
    "nn.test(x_test, y_onehot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31099753 -0.58776353  0.53529583  0.00175297] [ 0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_predict = x_test[0]\n",
    "\n",
    "print(x_predict, y_onehot_test[0])\n",
    "nn.predict(x_predict, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(range(100), accuracy, label=\"Accuracy\")\n",
    "plt.plot(range(100), loss, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4650487614017582046\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2366 - acc: 0.3100\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.2173 - acc: 0.3800\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.2073 - acc: 0.5500\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.1946 - acc: 0.6500\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.1782 - acc: 0.7100\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.1589 - acc: 0.6900\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.1440 - acc: 0.7300\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.1276 - acc: 0.7600\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.1191 - acc: 0.8500\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.1034 - acc: 0.8400\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 763us/step - loss: 0.1001 - acc: 0.8300\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0917 - acc: 0.8700\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0860 - acc: 0.9000\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0779 - acc: 0.9200\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0749 - acc: 0.9000\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0688 - acc: 0.9400\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0643 - acc: 0.9200\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0625 - acc: 0.9200\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0563 - acc: 0.9300\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0526 - acc: 0.9400\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0506 - acc: 0.9400\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0491 - acc: 0.9400\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0442 - acc: 0.9600\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0438 - acc: 0.9500\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0420 - acc: 0.9400\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0393 - acc: 0.9300\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0374 - acc: 0.9500\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0354 - acc: 0.9500\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0334 - acc: 0.9400\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0321 - acc: 0.9500\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 697us/step - loss: 0.0325 - acc: 0.9500\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 843us/step - loss: 0.0302 - acc: 0.9500\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 732us/step - loss: 0.0292 - acc: 0.9400\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0274 - acc: 0.9700\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0264 - acc: 0.9500\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0258 - acc: 0.9600\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0245 - acc: 0.9700\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0211 - acc: 0.9900\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0241 - acc: 0.9600\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 743us/step - loss: 0.0228 - acc: 0.9600\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0213 - acc: 0.9500\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.0226 - acc: 0.9600\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0211 - acc: 0.9600\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.0194 - acc: 0.9900\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.0184 - acc: 0.9700\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.0199 - acc: 0.9700\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 743us/step - loss: 0.0191 - acc: 0.9900\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0200 - acc: 0.9500\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0190 - acc: 0.9700\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0199 - acc: 0.9600\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0189 - acc: 0.9600\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0182 - acc: 0.9700\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0184 - acc: 0.9800\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0166 - acc: 0.9700\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0163 - acc: 0.9800\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0179 - acc: 0.9700\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 903us/step - loss: 0.0172 - acc: 0.9700\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0169 - acc: 0.9700\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0173 - acc: 0.9700\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.0157 - acc: 0.9800\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0169 - acc: 0.9800\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0170 - acc: 0.9700\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.0153 - acc: 0.9800\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0160 - acc: 0.9700\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0168 - acc: 0.9700\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0166 - acc: 0.9600\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0168 - acc: 0.9500\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0157 - acc: 0.9800\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0158 - acc: 0.9700\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 727us/step - loss: 0.0166 - acc: 0.9600\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 783us/step - loss: 0.0164 - acc: 0.9700\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0151 - acc: 0.9800\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.0150 - acc: 0.9700\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0157 - acc: 0.9700\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0155 - acc: 0.9800\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0153 - acc: 0.9700\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0155 - acc: 0.9700\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0144 - acc: 0.9800\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0136 - acc: 0.9700\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0157 - acc: 0.9500\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0154 - acc: 0.9700\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 652us/step - loss: 0.0154 - acc: 0.9600\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0152 - acc: 0.9700\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0137 - acc: 0.9900\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0156 - acc: 0.9600\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0136 - acc: 0.9900\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0147 - acc: 0.9800\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0135 - acc: 0.9800\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0150 - acc: 0.9800\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0141 - acc: 0.9700\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0137 - acc: 0.9800\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0142 - acc: 0.9700\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.0133 - acc: 0.9800\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0137 - acc: 0.9700\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.0137 - acc: 0.9700\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 657us/step - loss: 0.0135 - acc: 0.9800\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0130 - acc: 0.9800\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0132 - acc: 0.9800\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.0118 - acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17f90a3d940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_onehot_train, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
