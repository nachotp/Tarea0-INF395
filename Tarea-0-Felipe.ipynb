{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #se importa numpy como np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    def __init__(self, num_entrada, tam_capas, num_salida):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, \"tanh\"))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], \"tanh\", self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        \n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+1e-9)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado):\n",
    "        return (esperado-1/prediccion+(1+esperado)*(1/(1-prediccion)))  \n",
    "    \n",
    "    def cost(self, prediccion, esperado):\n",
    "        return np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def cost_derivative(self, prediccion, esperado):\n",
    "        return prediccion-esperado\n",
    "    \n",
    "    def train(self, entrada, salida, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            for input_v, output in resultList:\n",
    "                \n",
    "                self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                error_epoch = self.crossentropy(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                ce_derivate = self.crossentropy_derivative(self.capas[last].get_activaciones(), np.transpose(output))\n",
    "                delta = ce_derivate * self.capas[last].squash_derivative(self.capas[last].get_vector_z())\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                self.capas[last].actualizar_pesos(np.outer(self.capas[last-1].get_activaciones(), delta), tasa)\n",
    "                #print(self.capas[last].get_pesos())\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    self.capas[-l].actualizar_pesos(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa)\n",
    "                    \n",
    "            print(\"Error del epoch\",ciclo,\":\", str(np.round(np.mean(errores),7)))\n",
    "            \n",
    "            #Error de capa final, funcion costo\n",
    "            \n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            # evaluar error final (delta función costo)\n",
    "            error_epoch = self.crossentropy(self.capas[last].get_activaciones(), output)\n",
    "            errores.append(error_epoch)\n",
    "        print(\"Error del test:\", str(np.round(np.mean(errores),7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"tanh\":\n",
    "            self.squash = self.tanh\n",
    "            self.squash_derivative = self.tanh_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def tanh_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "        \n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "#transform target to one hot vector\n",
    "nn = RedNeuronal(4, [32, 16], 3)\n",
    "nn.train(x_train, y_onehot_train, 100, 0.1)\n",
    "print(\"TESTING\")\n",
    "nn.test(x_test, y_onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Referencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "### 2. Comparar back-propagation (BP) de Keras\n",
    "\n",
    "Keras es una de las librerı́as más populares para desarrollar nuevos modelos de redes neuronales o implementar eficientemente modelos conocidos con fines prácticos, puesto que ofrece una interfaz para poder trabajar de una manera mucho mas simple además de permitir también el manejo de configuraciones mas específicas.  \n",
    "Como actividad pedagógica ahora se les pide comparar el algoritmo implementado por ustedes con el de alto nivel de la librería __[keras](https://keras.io/)__ . Se les pedirá comparar sobre el mismo dataset con la misma arquitectura utilizada anteriormente, es decir, dos capas ocultas (con 32 y 16 neuronas respectivamente), 3 neuronas en la capa de salida con función de activación softmax, optimizador Gradiente Descentente (GD) con tasa de aprendizaje fija.\n",
    "\n",
    "<img src=\"https://i.imgur.com/hUjFUDU.png\" width=\"40%\" height=\"40%\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con Sigmoid y Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 30\n",
    "a = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=1)\n",
    "x =  np.arange(1, num_epochs+1, 1)\n",
    "\n",
    "accuracy = a.history['acc']\n",
    "loss = a.history['loss']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,8))\n",
    "plt.plot(x, accuracy, label=\"Accuracy\")\n",
    "plt.plot(x, loss, label=\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.title('Exactitud y pérdida según número de epoch', fontsize=18)\n",
    "plt.ylabel('Medición ', fontsize=18)\n",
    "plt.xlabel('N° de epoch', fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con Sigmoid y Median Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 30\n",
    "a = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=1)\n",
    "x =  np.arange(1, num_epochs+1, 1)\n",
    "\n",
    "accuracy = a.history['acc']\n",
    "loss = a.history['loss']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,8))\n",
    "plt.plot(x, accuracy, label=\"Accuracy\")\n",
    "plt.plot(x, loss, label=\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.title('Exactitud y pérdida según número de epoch', fontsize=18)\n",
    "plt.ylabel('Medición ', fontsize=18)\n",
    "plt.xlabel('N° de epoch', fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con Relu y Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 30\n",
    "a = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=1)\n",
    "x =  np.arange(1, num_epochs+1, 1)\n",
    "\n",
    "accuracy = a.history['acc']\n",
    "loss = a.history['loss']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,8))\n",
    "plt.plot(x, accuracy, label=\"Accuracy\")\n",
    "plt.plot(x, loss, label=\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.title('Exactitud y pérdida según número de epoch', fontsize=18)\n",
    "plt.ylabel('Medición ', fontsize=18)\n",
    "plt.xlabel('N° de epoch', fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con Relu y Median Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 30\n",
    "a = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=1)\n",
    "x =  np.arange(1, num_epochs+1, 1)\n",
    "\n",
    "accuracy = a.history['acc']\n",
    "loss = a.history['loss']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,8))\n",
    "plt.plot(x, accuracy, label=\"Accuracy\")\n",
    "plt.plot(x, loss, label=\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.title('Exactitud y pérdida según número de epoch', fontsize=18)\n",
    "plt.ylabel('Medición ', fontsize=18)\n",
    "plt.xlabel('N° de epoch', fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "### 3. Verificación numérica del gradiente para una componente\n",
    "\n",
    "En esta sección deberá verificar numéricamente el gradiente para los parámetros del modelo (que en este caso son los pesos de la red), que hasta ahora a definido de manera analítica en su programa, por ejemplo la derivada de $x^2$ es $2x$. Ahora deberá verificar estos cálculos usando la definición de gradiente.\n",
    "\n",
    "$$ \\nabla_{w} Loss = \\lim_{\\epsilon \\rightarrow 0} \\frac{Loss(w+ \\epsilon)-Loss(w)}{\\epsilon} $$\n",
    "\n",
    "Debido a que el *forward propagation* es relativamente fácil de implementar, se puede confiar en que se realizó de manera correcta, por lo que el cómputo del error (*loss*) debería ser correcto. Esto significa que podemos verificar el gradiente o la derivada analítica del error $\\frac{\\partial Loss}{\\partial w}$ comprobando que el resultado obtenido es similar (dentro de una tolerancia numérica, por ejemplo $10^6$) al valor que obtenemos aplicando la fórmula anterior. Naturalmente interpretaremos $\\lim_{\\epsilon \\rightarrow 0}$ como un valor \"*suficientemente pequeño*\" de $\\epsilon$.\n",
    "\n",
    "\n",
    "> a) Para un peso escogido aleatoriamente entre la primera capa de la red (*input*) y la primera capa oculta, calcule el valor del gradiente de la función de error para ambas funciones utilizadas (ayúdese mediante las funciones de *backward pass* implementadas anteriormente), luego compare y verifique con el valor numérico del gradiente mediante el procedimiento explicado anteriormente.\n",
    "\n",
    "> b) Vuelva a verificar el valor del gradiente para otros dos pesos escodigos aleatoriamente en la primera operación de la red. Compare y concluya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "### 4. Implementar *momentum* como variante\n",
    "\n",
    "En esta sección deberá construir, sin usar librerı́as, excepto eventualmente *numpy* para implementar operaciones básicas de algebra lineal, una variante del programa definido anteriormente ([sección 1](#primero)) que entrene la red utilizando *momentum* clásico.\n",
    "\n",
    "$$ v^{(t+1)} \\leftarrow \\mu v^{(t)} - \\eta \\nabla_{w^{(t)}} Loss \\\\\n",
    "w^{(t+1)} \\leftarrow w^{(t)} + v^{(t+1)}\n",
    "$$\n",
    "\n",
    "> *Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013, February). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).*\n",
    "\n",
    "\n",
    "Demuestre que su programa funciona en el mismo problema de clasificación presentado anteriormente, para esto, además deberá construir un gráfico de la función de error o pérdida (*loss*) *vs* el número de *epochs* y comentar/analizar la convergencia. ¿Es una mejora significativa? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-03849f4b36ffe0d8bdf51b058c316ec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-834da2a61df256c91ba57827da985638\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones\n",
    "\n",
    "* El valor de momentum m debe ser decidido a prueba y error. Su valor está entre 0 y 1\n",
    "* En caso de m cercano a 1, se debe reducir el learning rate. Combinar alto momentum y learning rate provocará que el algoritmo de pasos enormes y no llegue al mínimo óptimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    \n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    # act_inter: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\"\n",
    "    # cost_func: función de costo: \"crossentropy\" / \"mse\" \n",
    "    def __init__(self, num_entrada, tam_capas, num_salida, act_inter, cost_func):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, act_inter))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], act_inter, self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        if cost_func == \"crossentropy\":\n",
    "            self.cost = self.crossentropy\n",
    "            self.cost_derivative = self.crossentropy_derivative\n",
    "        elif cost_func == \"mse\":\n",
    "            self.cost = self.mse\n",
    "            self.cost_derivative = self.mse_derivative\n",
    "        \n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+1e-9)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado):\n",
    "        return (esperado-1/prediccion+(1+esperado)*(1/(1-prediccion)))  \n",
    "    \n",
    "    def mse(self, prediccion, esperado):\n",
    "        return np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def mse_derivative(self, prediccion, esperado):\n",
    "        return prediccion-esperado\n",
    "    \n",
    "    # train(entrada, salida, ciclos, tasa):\n",
    "    # entrada: lista de datos de entrada\n",
    "    # salida: lista de vectores de salida tipo onehot\n",
    "    # ciclos: cantidad de epochs\n",
    "    # tasa: tasa de aprendizaje \n",
    "    # retorna loss y accuracy\n",
    "    \n",
    "    def train(self, entrada, salida, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            for input_v, output in resultList:\n",
    "                \n",
    "                self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                error_epoch = self.crossentropy(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                ce_derivate = self.crossentropy_derivative(self.capas[last].get_activaciones(), np.transpose(output))\n",
    "                delta = ce_derivate * self.capas[last].squash_derivative(self.capas[last].get_vector_z())\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                self.capas[last].actualizar_pesos(np.outer(self.capas[last-1].get_activaciones(), delta), tasa)\n",
    "                #print(self.capas[last].get_pesos())\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    self.capas[-l].actualizar_pesos(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa)\n",
    "                    \n",
    "            print(\"Error del epoch\",ciclo,\":\", str(np.round(np.mean(errores),7)))\n",
    "            \n",
    "            #Error de capa final, funcion costo\n",
    "            \n",
    "            \n",
    "    def train_momentum(self, entrada, salida, ciclos, tasa,momentum):\n",
    "        primera_iteracion = False\n",
    "        lista_delta_pesos = list(np.zeros(self.cant_capas))\n",
    "        \n",
    "        \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        \n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            acertados = 0\n",
    "            for input_v, output in resultList:\n",
    "                #Poblar capa inicial\n",
    "                self.capas[0].set_capa(input_v, input_v) \n",
    "                \n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                prediccion = self.capas[last].get_activaciones()\n",
    "                error_epoch = self.cost(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                \n",
    "                if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                ce_derivate = self.cost_derivative(prediccion, np.transpose(output))\n",
    "                delta = ce_derivate * self.capas[last].squash_derivative(self.capas[last].get_vector_z())\n",
    "                \n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                delta_peso = np.outer(self.capas[last-1].get_activaciones(), delta)\n",
    "                \n",
    "                if primera_iteracion == False:\n",
    "                    self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                else:            \n",
    "                    self.capas[last].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[last]) \n",
    "            \n",
    "                #Actualizacion de lista de pesos\n",
    "                lista_delta_pesos[last] = tasa*delta_peso\n",
    "                \n",
    "\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    \n",
    "                    if primera_iteracion == False:\n",
    "                        self.capas[-l].actualizar_pesos(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa)\n",
    "                    else:\n",
    "                        self.capas[-l].actualizar_pesos_momentum(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa, momentum, lista_delta_pesos[-l])\n",
    "                    #Actualizacion de lista de pesos\n",
    "                    lista_delta_pesos[- l] = np.outer(self.capas[-l-1].get_activaciones(),delta)\n",
    "                #ya se recorrio la primera iteracion, cambiar flag\n",
    "                primera_iteracion = True\n",
    "                \n",
    "            loss.append(np.mean(errores))\n",
    "            accuracy.append(acertados/len(resultList))\n",
    "            print(\"Error del epoch\",ciclo,\":\", str(np.round(np.mean(errores),7)))\n",
    "        return loss, accuracy\n",
    "\n",
    "    # test(entrada, salida):\n",
    "    # entrada: lista de datos de entrada de test\n",
    "    # salida: Lista de datos esperados de test, en formato onehot vector\n",
    "    # retorna loss y accuracy\n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        acertados = 0\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            \n",
    "            prediccion = self.capas[last].get_activaciones()\n",
    "            \n",
    "            if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                    \n",
    "            # evaluar error final (delta función costo)\n",
    "            error_epoch = self.crossentropy(prediccion, output)\n",
    "            errores.append(error_epoch)\n",
    "            \n",
    "        print(\"Error del test:\", str(np.round(np.mean(errores),7)))\n",
    "        return np.mean(errores), acertados/len(resultList)\n",
    "\n",
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"arctan\":\n",
    "            self.squash = self.arctan\n",
    "            self.squash_derivative = self.arctan_derivative\n",
    "        elif tipo == \"relu\":\n",
    "            self.squash = self.relu\n",
    "            self.squash_derivative = self.relu_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def arctan(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def arctan_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        x= np.array(x)\n",
    "        return np.maximum(x, 0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        x = np.array(x)\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "        \n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_pesos_momentum(self, matriz, rate, momentum, delta_pesos_anterior):\n",
    "        #delta_pesos = rate*matriz  #ESTO ES SIN MOMENTUM\n",
    "        delta_pesos = (rate*matriz) + (momentum * delta_pesos_anterior)\n",
    "        \n",
    "        \n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= delta_pesos\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "        \n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Instanciar red neuronal\n",
    "nn_am = RedNeuronal(4, [32, 16], 3, \"arctan\", \"mse\")\n",
    "nn_am_sinmomentum = RedNeuronal(4, [32, 16], 3, \"arctan\", \"mse\")\n",
    "\n",
    "nn_sm = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "#nn_sc = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "nn_rm = RedNeuronal(4, [32, 16], 3, \"relu\", \"mse\")\n",
    "#nn_rc = RedNeuronal(4, [32, 16], 3, \"relu\", \"cross_entropy \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenar la red\n",
    "#loss_am_sinmomentum,accuracy_am_sinmomentum = nn_am_sinmomentum.train(x_train, y_onehot_train, 200, 0.1)\n",
    "\n",
    "loss_am,accuracy_am = nn_am.train_momentum(x_train, y_onehot_train, 100, 0.1, 0.9)\n",
    "\n",
    "loss_sm, accuracy_sm = nn_sm.train_momentum(x_train, y_onehot_train, 100, 0.1, 0.9)\n",
    "loss_rm,accuracy_rm = nn_rm.train_momentum(x_train, y_onehot_train, 100, 0.1, 0.9)\n",
    "\n",
    "#Testear\n",
    "print(\"TESTING\")\n",
    "nn_sm.test(x_test, y_onehot_test)\n",
    "nn_rm.test(x_test,y_onehot_test)\n",
    "nn_am.test(x_test,y_onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(range(100), accuracy_am, label=\"Accuracy\")\n",
    "plt.plot(range(100), loss_am, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(range(100), accuracy_sm, label=\"Accuracy\")\n",
    "plt.plot(range(100), loss_sm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(range(100), accuracy_rm, label=\"Accuracy\")\n",
    "plt.plot(range(100), loss_rm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* Momentum: https://www.willamette.edu/~gorr/classes/cs449/momrate.html\n",
    "* Momentum: https://distill.pub/2017/momentum/\n",
    "* Formula Momentum: https://visualstudiomagazine.com/articles/2017/08/01/neural-network-momentum.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
