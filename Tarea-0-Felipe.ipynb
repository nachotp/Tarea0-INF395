{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<div align='center'>\n",
    "###  Felipe González - 201273534-3 \n",
    "### Ignacio Tampe - 201573514-k \n",
    "</div>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back-propagation (BP) from *Scratch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "# Útil para graficar\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    \n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    # act_inter: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\"\n",
    "    # cost_func: función de costo: \"crossentropy\" / \"mse\" \n",
    "    def __init__(self, num_entrada, tam_capas, num_salida, act_inter, cost_func):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        self.log = dict()\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, act_inter))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], act_inter, self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        if cost_func == \"crossentropy\":\n",
    "            self.cost = self.crossentropy\n",
    "            self.cost_derivative = self.crossentropy_derivative\n",
    "        elif cost_func == \"mse\":\n",
    "            self.cost = self.mse\n",
    "            self.cost_derivative = self.mse_derivative\n",
    "        print(\"Red Neuronal creada\")\n",
    "        print(\"\\tCapas:\", num_entrada, tam_capas, num_salida)\n",
    "        print(\"\\tActivación interna:\", act_inter)\n",
    "        print(\"\\tFunción de costo:\", cost_func)\n",
    "\n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+epsilon)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado, squash_prime, vector_z):\n",
    "        return prediccion-esperado\n",
    "\n",
    "    def mse(self, prediccion, esperado):\n",
    "        return 0.5*np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def mse_derivative(self, prediccion, esperado, squash_prime, vector_z):\n",
    "        return (prediccion-esperado) * squash_prime(vector_z)\n",
    "    \n",
    "    # train(entrada, salida, ciclos, tasa):\n",
    "    # entrada: lista de datos de entrada\n",
    "    # salida: lista de vectores de salida tipo onehot\n",
    "    # ciclos: cantidad de epochs\n",
    "    # tasa: tasa de aprendizaje \n",
    "    # verbose: Booleano que indica progreso por epoch o solo el final\n",
    "    # momentum: agrega la posibilidad de entrenar con momentum, recibe el valor de éste.\n",
    "    # retorna loss y accuracy\n",
    "    def train(self, entrada, salida, ciclos, tasa, verbose = False, momentum = None):\n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        tiempo = []\n",
    "        print(\"Comienza entrenamiento de red\", end=\" \")\n",
    "        if momentum:\n",
    "            print(\"con momentum\", momentum)\n",
    "        else:\n",
    "            print()\n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            acertados = 0\n",
    "            primera_iter = True\n",
    "            lista_delta_pesos = list(np.zeros(self.cant_capas))\n",
    "            start = time.time()\n",
    "            for input_v, output in resultList:\n",
    "                #Poblar capa inicial\n",
    "                self.capas[0].set_capa(input_v, input_v) \n",
    "                \n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                prediccion = self.capas[last].get_activaciones()\n",
    "                error_epoch = self.cost(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                \n",
    "                if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                delta = self.cost_derivative(prediccion, np.transpose(output), self.capas[last].squash_derivative, self.capas[last].get_vector_z())\n",
    "                \n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                \n",
    "                delta_peso = np.outer(self.capas[last-1].get_activaciones(), delta)\n",
    "                \n",
    "                if momentum:\n",
    "                    if primera_iter:\n",
    "                        self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                    else:\n",
    "                        self.capas[last].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[last])\n",
    "                    lista_delta_pesos[last] = tasa*delta_peso\n",
    "                else:\n",
    "                    self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    delta_peso = np.outer(self.capas[-l-1].get_activaciones(),delta)\n",
    "                    \n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    if momentum:\n",
    "                        if primera_iter:\n",
    "                            self.capas[-l].actualizar_pesos(delta_peso, tasa)\n",
    "                        else:\n",
    "                            self.capas[-l].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[-l])\n",
    "                        #Actualizacion de lista de pesos\n",
    "                        lista_delta_pesos[-l] = np.outer(self.capas[-l-1].get_activaciones(), delta)\n",
    "                    else:\n",
    "                        self.capas[-l].actualizar_pesos(delta_peso, tasa)\n",
    "                        \n",
    "                primera_iter = True\n",
    "            loss.append(np.mean(errores))\n",
    "            accuracy.append(acertados/len(resultList))\n",
    "            tiempo.append(time.time()-start)\n",
    "            primera_ter = False\n",
    "            if verbose:\n",
    "                print(\"\\tError del epoch \"+str(ciclo)+\": loss\", str(np.round(np.mean(errores),5)),\"- acc\", acertados/len(resultList))\n",
    "        print(\"Resultado de training set de\", ciclos,\"epochs: loss\", np.mean(errores),\"- acc:\", acertados/len(resultList))\n",
    "        self.log['loss'] = loss\n",
    "        self.log['accuracy'] = accuracy\n",
    "        self.log['tiempo'] = tiempo\n",
    "\n",
    "    # test(entrada, salida):\n",
    "    # entrada: lista de datos de entrada de test\n",
    "    # salida: Lista de datos esperados de test, en formato onehot vector\n",
    "    # retorna loss y accuracy\n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        acertados = 0\n",
    "        lista_pred = []\n",
    "        lista_res = []\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            \n",
    "            prediccion = self.capas[last].get_activaciones()\n",
    "            \n",
    "            if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "            lista_pred.append(np.argmax(prediccion))\n",
    "            lista_res.append(np.argmax(output))\n",
    "            # evaluar error final (delta función costo)\n",
    "            error_epoch = self.crossentropy(prediccion, output)\n",
    "            errores.append(error_epoch)\n",
    "        \n",
    "        print(\"Error del test: loss\", str(np.round(np.mean(errores),5)),\"- acc\", acertados/len(resultList))\n",
    "        return lista_pred, lista_res\n",
    "    \n",
    "    def predict(self, entrada, categorias):\n",
    "        last = self.cant_capas-1\n",
    "        self.capas[0].set_capa(entrada, entrada)\n",
    "        \n",
    "        for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "        \n",
    "        prediccion = self.capas[last].get_activaciones()\n",
    "        \n",
    "        return categorias[np.argmax(prediccion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"arctan\":\n",
    "            self.squash = self.arctan\n",
    "            self.squash_derivative = self.arctan_derivative\n",
    "        elif tipo == \"relu\":\n",
    "            self.squash = self.relu\n",
    "            self.squash_derivative = self.relu_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def arctan(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def arctan_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        x= np.array(x)\n",
    "        return np.maximum(x, 0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        x = np.array(x)\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.exp(x-np.max(x))\n",
    "        return x / np.sum(x, axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate, momentum = 0, prev_delta_pesos = 0):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz + momentum*prev_delta_pesos\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "        \n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡Es hora de probar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=50)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "tags = ['setosa', 'versicolor', 'verginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar red neuronal\n",
    "nn_sm = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "#nn_sc = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"crossentropy\")\n",
    "#nn_sm_m = RedNeuronal(4, [32, 16 8], 3, \"sigmoid\", \"crossentropy\")# Instanciar red neuronal\n",
    "nn_sm = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "nn_sm_m = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenar la red \n",
    "nn_sm.train(x_train, y_onehot_train, 100, 0.1, verbose = False)\n",
    "loss_sm = nn_sm.log['loss']\n",
    "accuracy_sm = nn_sm.log['accuracy']\n",
    "tiempo_sm = nn_sm.log['tiempo']\n",
    "\n",
    "#nn_sc.train(x_train, y_onehot_train, 100, 0.1, verbose = False)\n",
    "#Testear\n",
    "print(\"\\nTESTING\")\n",
    "nn_sm.test(x_test, y_onehot_test)\n",
    "#nn_sc.test(x_test, y_onehot_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = x_test[5]\n",
    "\n",
    "print(\"Probando para\", tags[np.argmax(y_onehot_test[5])])\n",
    "print(\"Predicción de NN:\",nn_sm.predict(x_predict, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(15,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(accuracy_sm)), accuracy_sm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_sm)), loss_sm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(accuracy_sc)), accuracy_sc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_sc)), loss_sc, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Resultado real')\n",
    "    plt.xlabel('Resultado predicho')\n",
    "    \n",
    "pred, real = nn_sm.test(x_test, y_onehot_test)\n",
    "cnf_matrix = confusion_matrix(real, pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=tags, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "### 2. Comparar back-propagation (BP) de Keras\n",
    "\n",
    "Keras es una de las librerı́as más populares para desarrollar nuevos modelos de redes neuronales o implementar eficientemente modelos conocidos con fines prácticos, puesto que ofrece una interfaz para poder trabajar de una manera mucho mas simple además de permitir también el manejo de configuraciones mas específicas.  \n",
    "\n",
    "Como actividad pedagógica se comparó el algoritmo implementado en la sección anterior con el de alto nivel de la librería __[keras](https://keras.io/)__ . Se compararon los algoritmos sobre el mismo dataset con la misma arquitectura utilizada anteriormente, es decir, dos capas ocultas (con 32 y 16 neuronas respectivamente), 3 neuronas en la capa de salida con función de activación softmax, optimizador Descenso Gradiente Estocástico (SGD) con tasa de aprendizaje fija.\n",
    "\n",
    "Utilizando la arquitectura definida anteriormente, se realizaron las comparaciones teniendo en cuenta las siguientes combinaciones de funciones de activación y de costo:\n",
    ">* Sigmoid y Cross Entropy\n",
    ">* Sigmoid y Median Squared Error\n",
    ">* ReLU y Cross Entropy\n",
    ">* ReLU y Median Squared Error\n",
    "\n",
    "Los resultados fueron los siguientes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red utilizando Sigmoid y Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.1),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback para medir los tiempos por epoch de keras\n",
    "# Se utilizara en todas las combinaciones (sigmoid-cross entropy; sigmoid-mse, ReLu-cross entropy, ReLu-mse)\n",
    "import time\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self,epoch, logs={}):\n",
    "        self.starttime=time.time()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(time.time()-self.starttime)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de epochs que se utilizará en todas las combinaciones\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Entrenamiento de red utilizando keras. Función activación:  sigmoide  Función de costo: cross entropy\n",
    "tiempos_keras_sc = TimingCallback()\n",
    "keras_sc = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=0, callbacks=[tiempos_keras_sc])\n",
    "accuracy_keras_sc = keras_sc.history['acc']\n",
    "loss_keras_sc = keras_sc.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación Propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sc = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"crossentropy\")\n",
    "nn_sc.train(x_train, y_onehot_train, num_epochs, 0.1, verbose = False)\n",
    "loss_propia_sc = nn_sm.log['loss']\n",
    "accuracy_propia_sc = nn_sm.log['accuracy']\n",
    "tiempo_propia_sc = nn_sm.log['tiempo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1, figsize=(16,6))\n",
    "plt.suptitle(\"Accuracy y Loss según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_propia_sc)), accuracy_propia_sc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_propia_sc)), loss_propia_sc, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_keras_sc)), accuracy_keras_sc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_keras_sc)), loss_keras_sc, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Implementación Propia = \"+str(round(accuracy_propia_sc[-1]*100,2))+\"%\")\n",
    "print(\"Accuracy Keras = \"+str(round(accuracy_keras_sc[-1]*100,2))+\"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Loss Implementación Propia = \"+str(round(loss_propia_sc[-1],4)))\n",
    "print(\"Loss Keras = \"+str(round(loss_keras_sc[-1],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1, figsize=(16,6))\n",
    "\n",
    "plt.suptitle(\"Tiempos de ejecución por epoch según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(tiempo_propia_sc)), tiempo_propia_sc)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(tiempos_keras_sc.logs)), tiempos_keras_sc.logs)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "print(\"El tiempo total del entrenamiento con nuestra implementación fue de \"+str(round(sum(tiempo_propia_sc),2))+\" segundos\")\n",
    "print(\"El tiempo total del entrenamiento con keras fue de \"+str(round(sum(tiempos_keras_sc.logs),2))+\" segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis:\n",
    "\n",
    "\n",
    "**RELLENAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con Sigmoid y Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Entrenamiento de red utilizando keras. Función activación:  sigmoide  Función de costo: median squared error\n",
    "tiempos_keras_sm = TimingCallback()\n",
    "keras_sm = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=0, callbacks=[tiempos_keras_sm])\n",
    "accuracy_keras_sm = keras_sm.history['acc']\n",
    "loss_keras_sm = keras_sm.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sm = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "nn_sm.train(x_train, y_onehot_train, num_epochs, 0.1, verbose = False)\n",
    "loss_propia_sm = nn_sm.log['loss']\n",
    "accuracy_propia_sm = nn_sm.log['accuracy']\n",
    "tiempo_propia_sm = nn_sm.log['tiempo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16,6))\n",
    "plt.suptitle(\"Accuracy y Loss según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_propia_sm)), accuracy_propia_sm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_propia_sm)), loss_propia_sm, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_keras_sm)), accuracy_keras_sm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_keras_sm)), loss_keras_sm, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Implementación Propia = \"+str(round(accuracy_propia_sm[-1]*100,2))+\"%\")\n",
    "print(\"Accuracy Keras = \"+str(round(accuracy_keras_sm[-1]*100,2))+\"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Loss Implementación Propia = \"+str(round(loss_propia_sm[-1],4)))\n",
    "print(\"Loss Keras= \"+str(round(loss_keras_sm[-1],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16,6))\n",
    "plt.suptitle(\"Tiempos de ejecución por epoch según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(tiempo_propia_sm)), tiempo_propia_sm)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(tiempos_keras_sm.logs)), tiempos_keras_sm.logs)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "print(\"El tiempo total del entrenamiento con nuestra implementación fue de \"+str(round(sum(tiempo_propia_sm),2))+\" segundos\")\n",
    "print(\"El tiempo total del entrenamiento con keras fue de \"+str(round(sum(tiempos_keras_sm.logs),2))+\" segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con ReLU y Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.1),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Entrenamiento de red utilizando keras. Función activación:  relu  Función de costo: cross entropy\n",
    "tiempos_keras_rc = TimingCallback()\n",
    "keras_rc = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=0, callbacks=[tiempos_keras_rc])\n",
    "accuracy_keras_rc = keras_sm.history['acc']\n",
    "loss_keras_rc = keras_sm.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_rc = RedNeuronal(4, [32,16], 3, \"relu\", \"crossentropy\")\n",
    "nn_rc.train(x_train, y_onehot_train, num_epochs, 0.1, verbose = False)\n",
    "loss_propia_rc = nn_rc.log['loss']\n",
    "accuracy_propia_rc = nn_rc.log['accuracy']\n",
    "tiempo_propia_rc = nn_rc.log['tiempo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16,6))\n",
    "plt.suptitle(\"Accuracy y Loss según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_propia_rc)), accuracy_propia_rc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_propia_rc)), loss_propia_rc, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_keras_rc)), accuracy_keras_rc, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_keras_rc)), loss_keras_rc, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Implementación Propia = \"+str(round(accuracy_propia_rc[-1]*100,2))+\"%\")\n",
    "print(\"Accuracy Keras = \"+str(round(accuracy_keras_rc[-1]*100,2))+\"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Loss Implementación Propia = \"+str(round(loss_propia_rc[-1],4)))\n",
    "print(\"Loss Keras= \"+str(round(loss_keras_rc[-1],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16,6))\n",
    "plt.suptitle(\"Tiempos de ejecución por epoch según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(tiempo_propia_rc)), tiempo_propia_rc)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(tiempos_keras_rc.logs)), tiempos_keras_rc.logs)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "print(\"El tiempo total del entrenamiento con nuestra implementación fue de \"+str(round(sum(tiempo_propia_rc),2))+\" segundos\")\n",
    "print(\"El tiempo total del entrenamiento con keras fue de \"+str(round(sum(tiempos_keras_rc.logs),2))+\" segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con ReLU y Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_test.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.1),loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Entrenamiento de red utilizando keras. Función activación:  relu  Función de costo: mse\n",
    "tiempos_keras_rm = TimingCallback()\n",
    "keras_rm = model.fit(x_train, y_onehot_train, epochs=num_epochs, batch_size=1, verbose=0, callbacks=[tiempos_keras_rm])\n",
    "accuracy_keras_rm = keras_sm.history['acc']\n",
    "loss_keras_rm = keras_sm.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación Propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_rm = RedNeuronal(4, [32,16], 3, \"relu\", \"mse\")\n",
    "nn_rm.train(x_train, y_onehot_train, num_epochs, 0.1, verbose = False)\n",
    "loss_propia_rm = nn_rm.log['loss']\n",
    "accuracy_propia_rm = nn_rm.log['accuracy']\n",
    "tiempo_propia_rm = nn_rm.log['tiempo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16,6))\n",
    "plt.suptitle(\"Accuracy y Loss según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_propia_rm)), accuracy_propia_rm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_propia_rm)), loss_propia_rm, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(accuracy_keras_rm)), accuracy_keras_rm, label=\"Accuracy\")\n",
    "plt.plot(range(len(loss_keras_rm)), loss_keras_rm, label=\"Loss\")\n",
    "plt.ylabel('Medición ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy Implementación Propia = \"+str(round(accuracy_propia_rm[-1]*100,2))+\"%\")\n",
    "print(\"Accuracy Keras = \"+str(round(accuracy_keras_rm[-1]*100,2))+\"%\")\n",
    "print(\"\\n\")\n",
    "print(\"Loss Implementación Propia = \"+str(round(loss_propia_rm[-1],4)))\n",
    "print(\"Loss Keras= \"+str(round(loss_keras_rm[-1],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16,6))\n",
    "plt.suptitle(\"Tiempos de ejecución por epoch según tipo de implementación \\n \\n\", fontsize=23, y=1.07)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Implementación propia\",fontsize=18)\n",
    "plt.plot(range(len(tiempo_propia_rm)), tiempo_propia_rm)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Keras\",fontsize=18)\n",
    "plt.plot(range(len(tiempos_keras_rm.logs)), tiempos_keras_rm.logs)\n",
    "plt.ylabel('Tiempo [segundos] ', fontsize=13)\n",
    "plt.xlabel('N° de epoch', fontsize=13)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "print(\"El tiempo total del entrenamiento con nuestra implementación fue de \"+str(round(sum(tiempo_propia_rm),2))+\" segundos\")\n",
    "print(\"El tiempo total del entrenamiento con keras fue de \"+str(round(sum(tiempos_keras_rm.logs),2))+\" segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "### 3. Verificación numérica del gradiente para una componente\n",
    "\n",
    "En esta sección deberá verificar numéricamente el gradiente para los parámetros del modelo (que en este caso son los pesos de la red), que hasta ahora a definido de manera analítica en su programa, por ejemplo la derivada de $x^2$ es $2x$. Ahora deberá verificar estos cálculos usando la definición de gradiente.\n",
    "\n",
    "$$ \\nabla_{w} Loss = \\lim_{\\epsilon \\rightarrow 0} \\frac{Loss(w+ \\epsilon)-Loss(w)}{\\epsilon} $$\n",
    "\n",
    "Debido a que el *forward propagation* es relativamente fácil de implementar, se puede confiar en que se realizó de manera correcta, por lo que el cómputo del error (*loss*) debería ser correcto. Esto significa que podemos verificar el gradiente o la derivada analítica del error $\\frac{\\partial Loss}{\\partial w}$ comprobando que el resultado obtenido es similar (dentro de una tolerancia numérica, por ejemplo $10^6$) al valor que obtenemos aplicando la fórmula anterior. Naturalmente interpretaremos $\\lim_{\\epsilon \\rightarrow 0}$ como un valor \"*suficientemente pequeño*\" de $\\epsilon$.\n",
    "\n",
    "\n",
    "> a) Para un peso escogido aleatoriamente entre la primera capa de la red (*input*) y la primera capa oculta, calcule el valor del gradiente de la función de error para ambas funciones utilizadas (ayúdese mediante las funciones de *backward pass* implementadas anteriormente), luego compare y verifique con el valor numérico del gradiente mediante el procedimiento explicado anteriormente.\n",
    "\n",
    "> b) Vuelva a verificar el valor del gradiente para otros dos pesos escodigos aleatoriamente en la primera operación de la red. Compare y concluya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "### 4. Implementar *momentum* como variante\n",
    "\n",
    "En esta sección se construyó, sin usar librerı́as, excepto eventualmente *numpy* para implementar operaciones básicas de algebra lineal, una variante del programa definido anteriormente ([sección 1](#primero)) que entrene la red utilizando *momentum* clásico.\n",
    "\n",
    "$$ v^{(t+1)} \\leftarrow \\mu v^{(t)} - \\eta \\nabla_{w^{(t)}} Loss \\\\\n",
    "w^{(t+1)} \\leftarrow w^{(t)} + v^{(t+1)}\n",
    "$$\n",
    "\n",
    "> *Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013, February). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En redes neuronales se ocupan estrategias de optimización (como *Gradient descent* o *Stochastic gradient descent*) para minimizar el error de la función utilizada y así alcanzar el mínimo global. En un mundo ideal, el algoritmo siempre alcanzaría el mínimo global. \n",
    "<figure>\n",
    "    <img src=\"https://qph.ec.quoracdn.net/main-qimg-03849f4b36ffe0d8bdf51b058c316ec4\">\n",
    "</figure>    \n",
    "\n",
    "Sin embargo, en el mundo real las superficies son más complejas por lo que podría estár compuesta de varios mínimos locales. \n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-834da2a61df256c91ba57827da985638\">\n",
    "En este caso, un algoritmo como GD y SGD podría perfectamente atacarse en un mínimo local haciendo creer que se ha alcanzado el mínimo global. Para evitar esta situación, se utiliza el término de momentum $\\mu$ en la función objetivo, este valor varía entre $0$ y $1$ y permite incrementar el tamaño de los pasos dados hacia el punto mínimo tratando de saltar desde un mínimo local. \n",
    "\n",
    "**¿Qué valor de $\\mu$ debe escogerse?**  Un valor alto de momentum implicará idealmente que la convergencia suceda de manera rápida. Además el valor de $\\mu$ debe ser mayor al del *learning rate* $\\eta$, ya que si $\\mu$ y $\\eta$ son igualmente altos, probablemente se esquivará el punto mínimo global a pasos agigantados. Un pequeño valor de $\\mu$ no permitiría evitar los mínimos locales y además retardaría el *training* del sistema. **El valor de $\\mu$ utilizado fue de $0.9$ mientras que $\\eta$ tuvo un valor de $0.1$**\n",
    "\n",
    "Hay que señalar también que momentum permite suavizar las variaciones en casos donde hay un constante cambio de dirección. Por último, el valor de momentum puede ser escogido a prueba y error o utilizando *cross validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando backpropagation con momentum en una red con $n$ diferentes pesos $W_k$, la íesima corrección para el peso $W_k$ está dada por: <br/>\n",
    "\n",
    "$$ \\Delta W_k^{(i)} = \\eta \\frac{\\partial E}{\\partial W_k} + \\mu \\Delta W_k^{(i-1)}$$\n",
    "\n",
    "donde $\\frac{\\partial E }{\\partial W_k}$ representa a la variación del costo con respecto al peso $W_k$. Además $\\eta$ representa  el *learning rate* y $\\mu$ el momentum. *Para mayor información sobre la fórmula utilizada se puede consultar el siguiente documento: [Fast Learning Algorithms](http://page.mi.fu-berlin.de/rojas/neural/chapter/K8.pdf)* *(pág. 187)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demuestre que su programa funciona en el mismo problema de clasificación presentado anteriormente, para esto, además deberá construir un gráfico de la función de error o pérdida (*loss*) *vs* el número de *epochs* y comentar/analizar la convergencia. ¿Es una mejora significativa? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones\n",
    "\n",
    "* El valor de momentum m debe ser decidido a prueba y error. Su valor está entre 0 y 1\n",
    "* En caso de m cercano a 1, se debe reducir el learning rate. Combinar alto momentum y learning rate provocará que el algoritmo de pasos enormes y no llegue al mínimo óptimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    \n",
    "    # num_entrada: dimension de los vectores de entrada, en iris: 4\n",
    "    # tam_capas: lista de cantidad de neuronas por cada capa interna, en iris: [32, 16]\n",
    "    # num_salida: dimension de los vectores de salida, en iris: 3\n",
    "    # act_inter: función de activación de capas internas: \"sigmoid\" / \"relu\" / \"arctan\" / \"softmax\"\n",
    "    # cost_func: función de costo: \"crossentropy\" / \"mse\" \n",
    "    def __init__(self, num_entrada, tam_capas, num_salida, act_inter, cost_func):\n",
    "        self.num_entrada = num_entrada\n",
    "        self.tam_capas = tam_capas\n",
    "        self.num_salida = num_salida\n",
    "        self.capas = []\n",
    "        # iniciar todas las capas inicial - ocultas - final\n",
    "        self.capas.append(CapaNeuronal(num_entrada, act_inter))\n",
    "        for i in range(len(tam_capas)):\n",
    "            self.capas.append(CapaNeuronal(tam_capas[i], act_inter, self.capas[i].tam_capa))\n",
    "        self.capas.append(CapaNeuronal(num_salida, \"softmax\", self.capas[len(self.capas)-1].tam_capa))\n",
    "        self.cant_capas = len(self.capas)\n",
    "        if cost_func == \"crossentropy\":\n",
    "            self.cost = self.crossentropy\n",
    "            self.cost_derivative = self.crossentropy_derivative\n",
    "        elif cost_func == \"mse\":\n",
    "            self.cost = self.mse\n",
    "            self.cost_derivative = self.mse_derivative\n",
    "        \n",
    "    def crossentropy(self, prediccion, esperado, epsilon=1e-12): #El epsilon evita log(0)\n",
    "        prediccion = np.clip(prediccion, epsilon, 1. - epsilon)\n",
    "        N = prediccion.shape[0]\n",
    "        ce = -np.sum(np.sum(esperado*np.log(prediccion+1e-9)))/N\n",
    "        return ce\n",
    "    \n",
    "    def crossentropy_derivative(self, prediccion, esperado):\n",
    "        return (esperado-1/prediccion+(1+esperado)*(1/(1-prediccion)))  \n",
    "    \n",
    "    def mse(self, prediccion, esperado):\n",
    "        return np.dot(prediccion-esperado, prediccion-esperado)\n",
    "    \n",
    "    def mse_derivative(self, prediccion, esperado):\n",
    "        return prediccion-esperado\n",
    "    \n",
    "    # train(entrada, salida, ciclos, tasa):\n",
    "    # entrada: lista de datos de entrada\n",
    "    # salida: lista de vectores de salida tipo onehot\n",
    "    # ciclos: cantidad de epochs\n",
    "    # tasa: tasa de aprendizaje \n",
    "    # retorna loss y accuracy\n",
    "    \n",
    "    def train(self, entrada, salida, ciclos, tasa):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            for input_v, output in resultList:\n",
    "                \n",
    "                self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                error_epoch = self.crossentropy(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                ce_derivate = self.crossentropy_derivative(self.capas[last].get_activaciones(), np.transpose(output))\n",
    "                delta = ce_derivate * self.capas[last].squash_derivative(self.capas[last].get_vector_z())\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                self.capas[last].actualizar_pesos(np.outer(self.capas[last-1].get_activaciones(), delta), tasa)\n",
    "                #print(self.capas[last].get_pesos())\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    self.capas[-l].actualizar_pesos(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa)\n",
    "                    \n",
    "            print(\"Error del epoch\",ciclo,\":\", str(np.round(np.mean(errores),7)))\n",
    "            \n",
    "            #Error de capa final, funcion costo\n",
    "            \n",
    "            \n",
    "    def train_momentum(self, entrada, salida, ciclos, tasa,momentum):\n",
    "\n",
    "        \n",
    "        \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        \n",
    "        for ciclo in range(ciclos):\n",
    "            # Comienza 1 epoch\n",
    "            primera_iteracion = False\n",
    "            lista_delta_pesos = list(np.zeros(self.cant_capas))\n",
    "            random.shuffle(resultList)\n",
    "            errores = []\n",
    "            acertados = 0\n",
    "            for input_v, output in resultList:\n",
    "                #Poblar capa inicial\n",
    "                self.capas[0].set_capa(input_v, input_v) \n",
    "                \n",
    "                # Feed forward \n",
    "                for j in range(1, len(self.capas)):\n",
    "                    self.capas[j].feed_forward(self.capas[j-1])\n",
    "                \n",
    "                # evaluar error final (delta función costo)\n",
    "                prediccion = self.capas[last].get_activaciones()\n",
    "                error_epoch = self.cost(self.capas[last].get_activaciones(), output)\n",
    "                errores.append(error_epoch)\n",
    "                \n",
    "                if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                \n",
    "                #Backward pass capa final\n",
    "                ce_derivate = self.cost_derivative(prediccion, np.transpose(output))\n",
    "                delta = ce_derivate * self.capas[last].squash_derivative(self.capas[last].get_vector_z())\n",
    "                \n",
    "                self.capas[last].actualizar_bias(delta, tasa)\n",
    "                delta_peso = np.outer(self.capas[last-1].get_activaciones(), delta)\n",
    "                \n",
    "                if primera_iteracion == False:\n",
    "                    self.capas[last].actualizar_pesos(delta_peso, tasa)\n",
    "                else:            \n",
    "                    self.capas[last].actualizar_pesos_momentum(delta_peso, tasa, momentum, lista_delta_pesos[last]) \n",
    "            \n",
    "                #Actualizacion de lista de pesos\n",
    "                lista_delta_pesos[last] = tasa*delta_peso\n",
    "                \n",
    "\n",
    "                #Backward pass general\n",
    "                for l in range(2, self.cant_capas):\n",
    "                    z = self.capas[-l].get_vector_z()\n",
    "                    act_prime = self.capas[-l].squash_derivative(z)\n",
    "                    delta = np.dot(self.capas[-l+1].get_pesos(), delta.transpose()) * act_prime\n",
    "                    self.capas[-l].actualizar_bias(delta, tasa)\n",
    "                    \n",
    "                    if primera_iteracion == False:\n",
    "                        self.capas[-l].actualizar_pesos(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa)\n",
    "                    else:\n",
    "                        self.capas[-l].actualizar_pesos_momentum(np.outer(self.capas[-l-1].get_activaciones(),delta), tasa, momentum, lista_delta_pesos[-l])\n",
    "                    #Actualizacion de lista de pesos\n",
    "                    lista_delta_pesos[- l] = tasa * np.outer(self.capas[-l-1].get_activaciones(),delta)\n",
    "                #ya se recorrio la primera iteracion, cambiar flag\n",
    "                primera_iteracion = True\n",
    "                \n",
    "            loss.append(np.mean(errores))\n",
    "            accuracy.append(acertados/len(resultList))\n",
    "            print(\"Error del epoch\",ciclo,\":\", str(np.round(np.mean(errores),7)))\n",
    "        return loss, accuracy\n",
    "\n",
    "    # test(entrada, salida):\n",
    "    # entrada: lista de datos de entrada de test\n",
    "    # salida: Lista de datos esperados de test, en formato onehot vector\n",
    "    # retorna loss y accuracy\n",
    "    def test(self, entrada, salida):\n",
    "        # repetir para todas \n",
    "        last = self.cant_capas-1 # indice capa de salida\n",
    "        result = zip(entrada,salida)\n",
    "        resultList = list(result)\n",
    "        errores = []\n",
    "        acertados = 0\n",
    "        for input_v, output in resultList:\n",
    "            self.capas[0].set_capa(input_v, input_v) #Poblar capa inicial\n",
    "            # Feed forward \n",
    "            for j in range(1, len(self.capas)):\n",
    "                self.capas[j].feed_forward(self.capas[j-1])\n",
    "\n",
    "            \n",
    "            prediccion = self.capas[last].get_activaciones()\n",
    "            \n",
    "            if (np.argmax(prediccion) == np.argmax(output)):\n",
    "                    acertados += 1\n",
    "                    \n",
    "            # evaluar error final (delta función costo)\n",
    "            error_epoch = self.crossentropy(prediccion, output)\n",
    "            errores.append(error_epoch)\n",
    "            \n",
    "        print(\"Error del test:\", str(np.round(np.mean(errores),7)))\n",
    "        return np.mean(errores), acertados/len(resultList)\n",
    "\n",
    "class CapaNeuronal:\n",
    "    def __init__(self, tam_capa, tipo, tam_anterior = 0):\n",
    "        self.tam_capa = tam_capa\n",
    "        self.tam_anterior = tam_anterior\n",
    "        self.bias = 1\n",
    "        self.neuronas = [Neurona(tam_anterior) for i in range(tam_capa)] # lista de neuronas de la capa\n",
    "        if tipo == \"sigmoid\":\n",
    "            self.squash = self.sigmoid\n",
    "            self.squash_derivative = self.sigmoid_derivative\n",
    "        elif tipo == \"softmax\":\n",
    "            self.squash = self.softmax\n",
    "            self.squash_derivative = self.softmax_derivative\n",
    "        elif tipo == \"arctan\":\n",
    "            self.squash = self.arctan\n",
    "            self.squash_derivative = self.arctan_derivative\n",
    "        elif tipo == \"relu\":\n",
    "            self.squash = self.relu\n",
    "            self.squash_derivative = self.relu_derivative\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return np.array([np.where(i > 0, 1. / (1. + np.exp(-i)), np.exp(i) / (np.exp(i) + np.exp(0))) for i in x])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.multiply(self.sigmoid(x),(1-self.sigmoid(x)))\n",
    "    \n",
    "    def arctan(self, x):\n",
    "        return np.arctan(x)\n",
    "    \n",
    "    def arctan_derivative(self, x):\n",
    "        return np.power(np.cos(x),2)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        x= np.array(x)\n",
    "        return np.maximum(x, 0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        x = np.array(x)\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, x):\n",
    "        x = np.clip(x, -350, 350)\n",
    "        res = []\n",
    "        for i in x:\n",
    "            res.append(np.exp(i)*(np.sum(np.exp(x), axis=0)-np.exp(i))/(np.sum(np.exp(x), axis=0)**2))\n",
    "        #print(x)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_activaciones(self):\n",
    "        return np.array([neurona.activacion for neurona in self.neuronas])\n",
    "    \n",
    "    def get_vector_z(self):\n",
    "        return [neurona.z for neurona in self.neuronas]\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, capa_anterior):\n",
    "        pre_squash = []\n",
    "        activaciones = capa_anterior.get_activaciones()\n",
    "        vector_z = capa_anterior.get_vector_z()\n",
    "        for neurona in self.neuronas:\n",
    "            pesos = neurona.pesos\n",
    "            pre_squash.append(np.dot(pesos, activaciones) + neurona.bias)\n",
    "        self.set_capa(self.squash(pre_squash), pre_squash)\n",
    "        \n",
    "   \n",
    "    def get_pesos(self):\n",
    "        return np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "\n",
    "    def actualizar_pesos(self, matriz, rate):\n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= rate*matriz #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    def actualizar_pesos_momentum(self, matriz, rate, momentum, delta_pesos_anterior):\n",
    "        #delta_pesos = rate*matriz  #ESTO ES SIN MOMENTUM\n",
    "        delta_pesos = (rate*matriz) + (momentum * delta_pesos_anterior)\n",
    "        \n",
    "        \n",
    "        pesos = np.transpose(np.array([n.pesos for n in self.neuronas]))\n",
    "        pesos -= delta_pesos\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].pesos = pesos[:,i]\n",
    "    \n",
    "    \n",
    "    def actualizar_bias(self, vector_b, rate):\n",
    "        biases = np.array([n.bias for n in self.neuronas])\n",
    "        biases -= rate*vector_b #SE PUSO SUMA PORQUE BAJABA EL ERROR DEL EPOCH\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].bias = biases[i]\n",
    "    \n",
    "    #Funcion para poblar datos de entrada a la red.\n",
    "    def set_capa(self, activaciones, vector_z):\n",
    "        for i in range(len(self.neuronas)):\n",
    "            self.neuronas[i].set_activacion(activaciones[i], vector_z[i])\n",
    "            \n",
    "    def mostrar_capa(self):\n",
    "        for neurona in self.neuronas:\n",
    "            print(neurona, end=\"\\t\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "\n",
    "        \n",
    "class Neurona:\n",
    "    def __init__(self, tam_capa_anterior):\n",
    "        self.pesos = np.random.rand(tam_capa_anterior) # genera una lista de numeros aleatorios de tamaño \"tam_capa_anterior\"\n",
    "        self.bias = np.random.rand()\n",
    "        \n",
    "    def set_activacion(self, valor_a, valor_z):\n",
    "        self.activacion = valor_a\n",
    "        self.z = valor_z\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Activación: \"+str(self.activacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "x, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "y_onehot_train = keras.utils.to_categorical(y_train)\n",
    "y_onehot_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Instanciar red neuronal\n",
    "nn_am = RedNeuronal(4, [32, 16], 3, \"arctan\", \"mse\")\n",
    "nn_am_sinmomentum = RedNeuronal(4, [32, 16], 3, \"arctan\", \"mse\")\n",
    "\n",
    "nn_sm = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "#nn_sc = RedNeuronal(4, [32, 16], 3, \"sigmoid\", \"mse\")\n",
    "nn_rm = RedNeuronal(4, [32], 3, \"relu\", \"mse\")\n",
    "#nn_rc = RedNeuronal(4, [32, 16], 3, \"relu\", \"cross_entropy \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenar la red\n",
    "#loss_am_sinmomentum,accuracy_am_sinmomentum = nn_am_sinmomentum.train(x_train, y_onehot_train, 200, 0.1)\n",
    "\n",
    "loss_am,accuracy_am = nn_am.train_momentum(x_train, y_onehot_train, 100, 0.1, 0.9)\n",
    "\n",
    "loss_sm, accuracy_sm = nn_sm.train_momentum(x_train, y_onehot_train, 100, 0.1, 0.9)\n",
    "loss_rm,accuracy_rm = nn_rm.train_momentum(x_train, y_onehot_train, 200, 0.1, 0.9)\n",
    "\n",
    "#Testear\n",
    "print(\"TESTING\")\n",
    "nn_sm.test(x_test, y_onehot_test)\n",
    "nn_rm.test(x_test,y_onehot_test)\n",
    "nn_am.test(x_test,y_onehot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_rm = RedNeuronal(4, [32,16], 3, \"relu\", \"mse\")\n",
    "\n",
    "loss_rm,accuracy_rm = nn_rm.train_momentum(x_train, y_onehot_train, 100, 0.1, 0.9)\n",
    "\n",
    "nn_rm.test(x_test,y_onehot_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(range(100), accuracy_am, label=\"Accuracy\")\n",
    "plt.plot(range(100), loss_am, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(range(100), accuracy_sm, label=\"Accuracy\")\n",
    "plt.plot(range(100), loss_sm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(range(100), accuracy_rm, label=\"Accuracy\")\n",
    "plt.plot(range(100), loss_rm, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* Momentum: https://www.willamette.edu/~gorr/classes/cs449/momrate.html\n",
    "* Momentum: https://distill.pub/2017/momentum/\n",
    "* Formula Momentum: https://visualstudiomagazine.com/articles/2017/08/01/neural-network-momentum.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
